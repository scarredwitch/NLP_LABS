{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constraint: Only use our code (not other code....)\n",
    "\n",
    "1. I guess you already try a bigger corpus\n",
    "2. I guess you already try window size 2\n",
    "3. I guess you already have skipgram, skipgram(neg), cbow, glove\n",
    "\n",
    "Do this:\n",
    "1. Compare them based on syntactic accuracy and semantic accuracy, similar to how is done in https://nlp.stanford.edu/pubs/glove.pdf (see Table 2) - NO NEED to try 1000 or 300 embed size.....I just want you to learn how to do experiment.....\n",
    "2. Try to find a correlation with just ONE similarity dataset (which humans judge how similar is two words.....)\n",
    "\n",
    "Point criteria:\n",
    "0:  Not done\n",
    "1: ok\n",
    "2: with comments / explanation / figures just like how Chaky explain thing....."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some suggestions for the assignment report:\n",
    "1. Write a brief conclusion on your findings at the end of the report.\n",
    "2. Clear long outputs in the notebook before submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\Sirikit\n",
      "[nltk_data]     Joshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#using real corpus from nltk\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')\n",
    "corpus = nltk.corpus.brown.sents(categories=['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1751\n"
     ]
    }
   ],
   "source": [
    "#let's check the size of the corpus\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['it', 'is', 'not', 'news', 'that', 'nathan', 'milstein', 'is', 'a', 'wizard', 'of', 'the', 'violin', '.'], ['certainly', 'not', 'in', 'orchestra', 'hall', 'where', 'he', 'has', 'played', 'countless', 'recitals', ',', 'and', 'where', 'thursday', 'night', 'he', 'celebrated', 'his', '20th', 'season', 'with', 'the', 'chicago', 'symphony', 'orchestra', ',', 'playing', 'the', 'brahms', 'concerto', 'with', 'his', 'own', 'slashing', ',', 'demon-ridden', 'cadenza', 'melting', 'into', 'the', 'high', ',', 'pale', ',', 'pure', 'and', 'lovely', 'song', 'with', 'which', 'a', 'violinist', 'unlocks', 'the', 'heart', 'of', 'the', 'music', ',', 'or', 'forever', 'finds', 'it', 'closed', '.'], ['there', 'was', 'about', 'that', 'song', 'something', 'incandescent', ',', 'for', 'this', 'brahms', 'was', 'milstein', 'at', 'white', 'heat', '.'], ['not', 'the', 'noblest', 'performance', 'we', 'have', 'heard', 'him', 'play', ',', 'or', 'the', 'most', 'spacious', ',', 'or', 'even', 'the', 'most', 'eloquent', '.'], ['those', 'would', 'be', 'reserved', 'for', 'the', \"orchestra's\", 'great', 'nights', 'when', 'the', 'soloist', 'can', 'surpass', 'himself', '.']]\n"
     ]
    }
   ],
   "source": [
    "#1. Tokenize\n",
    "corpus_tokenized = [[word.lower() for word in sent] for sent in corpus]\n",
    "print(corpus_tokenized[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['smoothness', 'brahms', 'nor', 'bang-sashes', 'adaptation']\n"
     ]
    }
   ],
   "source": [
    "#2. numericalize\n",
    "\n",
    "#2.1 get all the unique words\n",
    "#we want to flatten this (basically merge all list)\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs  = list(set(flatten(corpus_tokenized)))  #vocabs is a term defining all unique words your system know\n",
    "print(vocabs[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.2 assign id to all these vocabs\n",
    "word2index = {v: idx+1 for idx, v in enumerate(vocabs)}\n",
    "\n",
    "#add <UNK>, which is a very normal token exists in the world\n",
    "vocabs.append('<UNK>') #chaky, can it be ##UNK, or UNKKKKKK, or anything\n",
    "\n",
    "word2index['<UNK>'] = 0 \n",
    "# word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create index2word dictionary\n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "# index2word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a9409b9deca2e32b1801514822bf60b5f36ee24c0efb3dea589c0a4c1325f8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

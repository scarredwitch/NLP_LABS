## Research Paper summaries for Natural Language Processing
____

## 1. Gmail Smart Compose: Real-Time Assisted Writing

| Topic  | Gmail Smart Compose: Real-Time Assisted Writing |
|--------------|--------------------------------------------------------------------------------------------------------|
| Hypothesis question | Does the implementation of a large-scale neural language model and the use of state-of-the-art machine learning techniques for language model training result in a successful system for generating real-time suggestions for reducing repetitive typing in email writing? |
| Key Related Work | 1. Neural network-based language models: The paper highlights the shift from traditional n-gram models to neural network-based language models in NLP and ASR systems. The latter has improved state-of-the-art performance in these systems. <br /> 2. Serving large-scale neural language models: The paper discusses the challenges of serving large-scale neural language models, which are heavy-weight, computationally expensive and stateful, compared to traditional n-gram models that are amenable to large-scale distributed serving. <br /> 3. Smart Compose vs. Smart Reply: The paper contrasts Smart Compose with Google's Smart Reply, a system for generating automated response suggestions for emails. Smart Compose is more flexible and sensitive to context, and generates suggestions for every sentence prefix in real-time, while Smart Reply only generates responses once for each email. <br /> 4. Personalization in language models: The paper mentions that Smart Compose has been extended for personalization, which is a critical feature in language model adaptation techniques. The approach used is a linear interpolation between an n-gram and recurrent neural network language model. <br /> 5. Multilingual models in NLP: The paper notes the importance of multilingual models in NLP, specifically for tasks like machine translation, and mentions that Smart Compose has been extended for multilingual use. The approach used is inspired by using wordpieces and shared models for all languages, which is similar to the work of Johnson et al. in this field. |
| Method |There are three different models are discussed for generating these suggestions: <br />1 .Language Model A (LM-A): This model treats the problem as a language modeling task, where the input to the model is the current email body prefix (the text the user has already written). The contextual information (subject, previous email body, date, time, and locale) is encoded and combined with the input sequence at each time step to better adapt the model to the current email composition environment. <br /> 2.Language Model B (LM-B): In this model, the subject, previous email body, and current email body are combined into a single sequence and used as the input to the language model. This approach is simpler in terms of model structure, but also leads to a much longer sequence length.<br /> 3. Sequence-to-Sequence Model (Seq2Seq): This model formulates the problem as a sequence-to-sequence prediction task similar to neural machine translation. The source sequence is the concatenation of the subject and previous email body (if there is one), and the target sequence is the current email body. Through the attention mechanism, a Seq2Seq model has the potential advantage of better understanding the context information.<br /> The method described is the process of generating email suggestions using the Smart Compose tool. The tool utilizes a language model to predict the next word in an email. At inference time, the necessary contextual information (subject, previous email, etc.) is fed into the model and a beam search algorithm is used to generate n best suggestions. The beam search algorithm works by maintaining a heap of m best candidate sequences. At each step of the beam search, new candidate sequences are generated by extending each candidate sequence by one token and adding them to the heap. The heap is then pruned to keep only the m best candidates. The beam search ends when no new candidate sequences are added or when the candidate sequence reaches a predefined maximum output sequence length. The top suggestion is provided to the user only when the model is confident enough about the suggestion, which is determined by a confidence score based on a length-normalized log conditional probability. The confidence score is used as the value for the ordering within the candidate heap during the beam search.|
| Evaluation Matrix |There are 2 evaluation matrix: 1. Log Perplexity 2. ExactMatch <br /> Datasets : Previous e-mail in case the composed e-mail was a response, Subject of the e-mail, Date and time of the composed e-mail, Locale of the user composing the e-mail. <br />
|  Results | 1. for LM-A : Transformer-1536-8192 has lower Log Perplexity Here,  1536 is the layer and 8192 is the hidden size and Transformer 1536-8192 has the better in extact match.   <br /> 2. LM-B Transformer-2048-8192 has lower Test Log Perplexity.<br /> 3. for Results using the Seq2Seq approach BiLSTM-6-LSTM-8-1024 has lower Log Perplexity.
| Conclusion | The Smart Compose system aims to improve the writing experience of Gmail users by providing real-time, context-dependent, and diverse suggestions as they type. The system has been designed to overcome challenges such as model design, evaluation, serving, privacy, and fairness, and has been enhanced with features such as personalization and multilingual support. The authors of the paper have conducted extensive experiments comparing the performance of different model and serving architectures and have demonstrated that the Smart Compose system is effective in improving the writing experience for Gmail users.|

____
____

## 2. A Fast and Accurate Dependency Parser using Neural Networks

### Authors: Danqi Chen and Christopher D. Manning

| Topic  | A Fast and Accurate Dependency Parser using Neural Networks |
|--------------|--------------------------------------------------------------------------------------------------------|
| Problem | Dependency Parser cannot handle complex real-world sentences in a fast and accurate manner |
| Key Related Work | 1. Koo et al. (2008) demonstrated improved parsing performance through techniques like word class features.<br /> 2. Bohnet (2010) reports inefficiency of his baseline parser in terms of feature extraction. <br /> 3. Collobert et al. (2011) successfully displayed the efficiency of distributed word representations in NLP task like POS tagging.|
| Method |Train a neural network classifier to make parsing decisions within a transition-based dependency parser. Introduce a novel activation function for this neural network that captures higher-order interaction features. |
|  Results | 1. Fast computation while achieving 2% improvement in UAS and LAS on both English and Chinese datasets. <br /> 2. Outperforms other greedy parsers using sparse indicator features in both accuracy and speed.|
| Future work | Authors developed a parser that outperforms current parsers and made a significant contribution to field of NLP. In future, authors hope to combine this neural network based classifier with search based models to further improve accuracy.|

____
____

## 3. Neural Machine Translation for Paraphrase Generation

### Authors: Alex Sokolov, Denix Filimonov

| Topic  | Neural Machine Translation for Paraphrase Generation |
|--------------|--------------------------------------------------------------------------------------------------------|
| Problem | Alexa Skill Kit (ASK) work flow's accuracy and user experience greatly depends on the data provided by the skill developer and manual annotations are expensive and time consuming. |
| Proposed Solution |asdfadfasdfa |
| Key Related Work | 1. Cho et al. (2014) developed an encoder-decoder architecture where encoder encoded source language to a vector representation with last hidden state containing complete sentence information. The decoder output one word at a time getting additional context each step by taking previous generated word as input. <br /> 2. Sutskever et al. (2014) demonstrated that n-best paraphrase hypotheses were generated through left-to-right beam search during decoding. <br /> 3. Schuster and Paliwal (1997) demonstrated the improved performance of bidirectional LSTM on longer sequences. This is utilized in the paper for the first layer where LSTM hidden states of each timestep summarizes preceding and following words. |
| Method |Train a neural network classifier to make parsing decisions within a transition-based dependency parser. Introduce a novel activation function for this neural network that captures higher-order interaction features. |
|  Results | 1. Fast computation while achieving 2% improvement in UAS and LAS on both English and Chinese datasets. <br /> 2. Outperforms other greedy parsers using sparse indicator features in both accuracy and speed.|
| Future work | Authors developed a parser that outperforms current parsers and made a significant contribution to field of NLP. In future, authors hope to combine this neural network based classifier with search based models to further improve accuracy.|




solution: machine translation inspired encoder-decoder deep recurrent neural network
____
____
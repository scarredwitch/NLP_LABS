 ## Research Paper summaries for Natural Language Processing

## 1. Gmail Smart Compose: Real-Time Assisted Writing

| Topic  | Gmail Smart Compose: Real-Time Assisted Writing |
|--------------|--------------------------------------------------------------------------------------------------------|
| Hypothesis question | Does the implementation of a large-scale neural language model and the use of state-of-the-art machine learning techniques for language model training result in a successful system for generating real-time suggestions for reducing repetitive typing in email writing? |
| Key Related Work | 1. Neural network-based language models: The paper highlights the shift from traditional n-gram models to neural network-based language models in NLP and ASR systems. The latter has improved state-of-the-art performance in these systems. <br /> 2. Serving large-scale neural language models: The paper discusses the challenges of serving large-scale neural language models, which are heavy-weight, computationally expensive and stateful, compared to traditional n-gram models that are amenable to large-scale distributed serving. <br /> 3. Smart Compose vs. Smart Reply: The paper contrasts Smart Compose with Google's Smart Reply, a system for generating automated response suggestions for emails. Smart Compose is more flexible and sensitive to context, and generates suggestions for every sentence prefix in real-time, while Smart Reply only generates responses once for each email. <br /> 4. Personalization in language models: The paper mentions that Smart Compose has been extended for personalization, which is a critical feature in language model adaptation techniques. The approach used is a linear interpolation between an n-gram and recurrent neural network language model. <br /> 5. Multilingual models in NLP: The paper notes the importance of multilingual models in NLP, specifically for tasks like machine translation, and mentions that Smart Compose has been extended for multilingual use. The approach used is inspired by using wordpieces and shared models for all languages, which is similar to the work of Johnson et al. in this field. |
| Method |There are three different models are discussed for generating these suggestions: <br />1 .Language Model A (LM-A): This model treats the problem as a language modeling task, where the input to the model is the current email body prefix (the text the user has already written). The contextual information (subject, previous email body, date, time, and locale) is encoded and combined with the input sequence at each time step to better adapt the model to the current email composition environment. <br /> 2.Language Model B (LM-B): In this model, the subject, previous email body, and current email body are combined into a single sequence and used as the input to the language model. This approach is simpler in terms of model structure, but also leads to a much longer sequence length.<br /> 3. Sequence-to-Sequence Model (Seq2Seq): This model formulates the problem as a sequence-to-sequence prediction task similar to neural machine translation. The source sequence is the concatenation of the subject and previous email body (if there is one), and the target sequence is the current email body. Through the attention mechanism, a Seq2Seq model has the potential advantage of better understanding the context information.<br /> The method described is the process of generating email suggestions using the Smart Compose tool. The tool utilizes a language model to predict the next word in an email. At inference time, the necessary contextual information (subject, previous email, etc.) is fed into the model and a beam search algorithm is used to generate n best suggestions. The beam search algorithm works by maintaining a heap of m best candidate sequences. At each step of the beam search, new candidate sequences are generated by extending each candidate sequence by one token and adding them to the heap. The heap is then pruned to keep only the m best candidates. The beam search ends when no new candidate sequences are added or when the candidate sequence reaches a predefined maximum output sequence length. The top suggestion is provided to the user only when the model is confident enough about the suggestion, which is determined by a confidence score based on a length-normalized log conditional probability. The confidence score is used as the value for the ordering within the candidate heap during the beam search.|
| Evaluation Matrix |There are 2 evaluation matrix: 1. Log Perplexity 2. ExactMatch <br /> Datasets : Previous e-mail in case the composed e-mail was a response, Subject of the e-mail, Date and time of the composed e-mail, Locale of the user composing the e-mail. <br />
|  Results | 1. for LM-A : Transformer-1536-8192 has lower Log Perplexity Here,  1536 is the layer and 8192 is the hidden size and Transformer 1536-8192 has the better in extact match.   <br /> 2. LM-B Transformer-2048-8192 has lower Test Log Perplexity.<br /> 3. for Results using the Seq2Seq approach BiLSTM-6-LSTM-8-1024 has lower Log Perplexity.
| Conclusion | The Smart Compose system aims to improve the writing experience of Gmail users by providing real-time, context-dependent, and diverse suggestions as they type. The system has been designed to overcome challenges such as model design, evaluation, serving, privacy, and fairness, and has been enhanced with features such as personalization and multilingual support. The authors of the paper have conducted extensive experiments comparing the performance of different model and serving architectures and have demonstrated that the Smart Compose system is effective in improving the writing experience for Gmail users.|

____
____

## 2. A Fast and Accurate Dependency Parser using Neural Networks

### Authors: Danqi Chen and Christopher D. Manning

| Topic  | A Fast and Accurate Dependency Parser using Neural Networks |
|--------------|--------------------------------------------------------------------------------------------------------|
| Problem | Dependency Parser cannot handle complex real-world sentences in a fast and accurate manner |
| Key Related Work | 1. Koo et al. (2008) demonstrated improved parsing performance through techniques like word class features.<br /> 2. Bohnet (2010) reports inefficiency of his baseline parser in terms of feature extraction. <br /> 3. Collobert et al. (2011) successfully displayed the efficiency of distributed word representations in NLP task like POS tagging.|
| Method |Train a neural network classifier to make parsing decisions within a transition-based dependency parser. Introduce a novel activation function for this neural network that captures higher-order interaction features. |
|  Results | 1. Fast computation while achieving 2% improvement in UAS and LAS on both English and Chinese datasets. <br /> 2. Outperforms other greedy parsers using sparse indicator features in both accuracy and speed.|
| Future work | Authors developed a parser that outperforms current parsers and made a significant contribution to field of NLP. In future, authors hope to combine this neural network based classifier with search based models to further improve accuracy.|

____
____

## 3. Neural Machine Translation for Paraphrase Generation

### Authors: Alex Sokolov, Denix Filimonov

| Topic  | Neural Machine Translation for Paraphrase Generation |
|--------------|--------------------------------------------------------------------------------------------------------|
| Problem | Alexa Skill Kit (ASK) work flow's accuracy and user experience greatly depends on the data provided by the skill developer and manual annotations are expensive and time consuming. |
| Proposed Solution | Machine Translation inspired encoder-decoder deep recurrent neural network |
| Key Related Work | 1. Cho et al. (2014) developed an encoder-decoder architecture where encoder encoded source language to a vector representation with last hidden state containing complete sentence information. The decoder output one word at a time getting additional context each step by taking previous generated word as input. <br /> 2. Sutskever et al. (2014) demonstrated that n-best paraphrase hypotheses were generated through left-to-right beam search during decoding. <br /> 3. Schuster and Paliwal (1997) demonstrated the improved performance of bidirectional LSTM on longer sequences. This is utilized in the paper for the first layer where LSTM hidden states of each timestep summarizes preceding and following words. <br /> 4. Pennington et al. (2014) showcased how GloVe embedding could reduce the model parameter space giving more robustness to rare words and synonyms.|
| Method | Followed the architecture of Cho et al. (2014) with GloVe embedding and bidirectional LSTM. A standard MT model was trained for English to French parallel corpus as well as training on in-domain English paraphrase corpus following three training schemes. |
|  Results | 1. Quantitative Results - All tested skills have four or more times improvement on the bigram coverage score compared to their baselines. <br /> 2. Qualitative Analysis - Limited vocabulary and shorter length of in-domain sentences result in smaller word error rate and better translation quality than out-of-domain corpora. The paraphrase model with copying mechanism has increased output diversity but only conserves intent and slots. In contrast, the one without it produces better grammar sentences and even propagates correct entities sometimes. |
| Future work | 1. Tagging existing French corpus or translating the in-domain data to French (with slots). <br /> 2. Decoupling decoder training from encoder training by conditioning it on input slot sequence instead. |

____
____
## 4. Prompt Combines Paraphrase: Teaching Pre-trained Models to Understand Rare Biomedical Words

### Authors: Haochun Wang, Chi Liu, Nuwa Xi, Sendong Zhao, Meizhi Ju, Shiwei Zhang, Ziheng Zhang, Yefeng Zheng, Bing Qin and Ting Liu
| Topic | Prompt Combines Paraphrase: Teaching Pre-trained Models to Understand Rare Biomedical Words |
|-----|--------------------------|
| Problems | Prompt-based fine-tuning of pre-trained models has not been thoroughly investigated in the biomedical domain. |
| Proposed Solution | Augmenting prompt with definition of rare biomedical words to help the model learn their meaning and usage. |
| Related Work | 1. Representation learning of rare words in pretrained models<br />&emsp;- vocabulary list of words in NLP follows Zipf distribution where few words have high frequency while majority of the words are rarely used.<br />    &emsp;- approximating embeddings for rare words using a single token<br />2. Biomedical pre-trained models<br />&emsp;- researchers have developed domain-specific vocabularies, guided pre-trained models with domain knowledge and learned clinical word embeddings<br />3. Tuning pre-trained models with prompt.<br />&emsp;- discrete prompts -> convert downstream tasks into close question formats without requiring additional parameters<br />&emsp;- continuous prompts -> insert prompt embeddings into models which can perform better but require additional training costs and reduce explainability.|
| Method | 1. Rare Words <br />&emsp;- context-specific rare word identification is necessary.<br />&emsp;- original semantics of rare words may not be preserved after tokenization.<br />2. Selection of Rare Biomedical Words<br />&emsp;- The authors use three widely used biomedical corpora - PubMed abstract, PubMed Central (PMC) full-text, and MIMIC-III dataset to obtain the frequency of each word in the pre-training phase, and consider rare words from biomedical domain due to their domain-specific distribution and task-specific importance.<br /> 3.  Selection of Paraphrases<br />&emsp;- exclude rare biomedical words with more than one corresponding paraphrase to avoid introducing noise from inappropriate paraphrases.<br />&emsp;- ignore paraphrases that contain additional rare words whose frequencies are below the set threshold, as this only replaces one rare word with another and does not add much semantic value. <br />4. Prompt-based Fine-Tuning with Paraphrases<br />&emsp;- seek dictionaries for corresponding paraphrases when encountering new words to guide the pre-trained models with paraphrases.|
| Experiment | Six pre-trained models in both general and biomedical domains i.e. <br />&emsp;1) BERT-Large <br />&emsp;2)  RoBERTa Large <br />&emsp;3)BioBERT-Base <br />&emsp;4) PubMedBERT-Base <br />&emsp;5) SciBERT-Base <br />&emsp;6) BC-RoBERTa-Large (Biomedical-Clinical RoBERTa-Large) <br /> In all the model learning rate is set to be 1*10^-5 and batch size 2 and max epoch is 10. |
| Results | 1. Results on MedNLI<br />&emsp;- pre-trained models with paraphrases for rare biomedical words can outperform the baselines in all cases and can bring about 6% improvement on average for fewshot learning with 16 training samples and 2% with 256 training samples.<br />2. Results on MedSTS<br />&emsp;- similar to MedNLI in terms of improved performance, for some cases, statistical significance is not as stable as that on MedNLI |
| Discussion | Train with more samples to achieve greater improvement results |

____
____
## 5. The Power of Scale for Parameter-Efficient Prompt Tuning

### Authors: Brian Lester, Rami Al-Rfou, Noah Constant
| Topic | The Power of Scale for Parameter-Efficient Prompt Tuning |
|-----|--------------------------|
| Problems | Large models are computationally costly to share and serve where a separate copy of the model is required for each downstream task. Prompt-based adaptation has drawbacks such as - task description is error-prone and requires human involvement.|
| Proposed Solution | 1. Single generalist model can simultaneously serve for multiple downstream tasks. <br />2. Conditioning a frozen model with soft prompts shows many benefits in terms of robustness in domain transfer and enables efficient "prompt ensembling".|
| Related Work | 1. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts <br />&emsp;- Shin et al. (2020) propose a search algorithm over the discrete space of words guided by downstream application training data.<br />&emsp;- This technique outperforms manual prompt design but there is still a gap relative to model tuning.<br />2. Prefix-tuning: Optimizing continuous prompts for generation <br />&emsp;- Li and Liang (2021) show strong results on generative tasks with their proposed method 'prefix tuning' where the model parameters are frozen and the error is backpropagated during tuning.<br />3. WARP: Word-level Adversarial ReProgramming<br />&emsp;- Hambardzumyan et al. (2021) simplify previous research by restricting trainable parameters to the input and output sub-networks of an MLM with reasonable results on classification tasks. |
| Related Work | 1. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts <br /><ul><li>Shin et al. (2020) propose a search algorithm over the discrete space of words guided by downstream application training data.</li><li>This technique outperforms manual prompt design but there is still a gap relative to model tuning.</li></ul><br />2. Prefix-tuning: Optimizing continuous prompts for generation <br /><ul><li>Li and Liang (2021) show strong results on generative tasks with their proposed method 'prefix tuning' where the model parameters are frozen and the error is backpropagated during tuning.</li></ul><br />3. WARP: Word-level Adversarial ReProgramming<br /><ul><li>Hambardzumyan et al. (2021) simplify previous research by restricting trainable parameters to the input and output sub-networks of an MLM with reasonable results on classification tasks.</li></ul>|
| Method | <ul><li>item1</li><li>item2</li></ul>|
| Experiment | |
| Results | |
| Discussion | |
## Reading Assignment - 1

## 1. Gmail Smart Compose: Real-Time Assisted Writing

| Topic  | Gmail Smart Compose: Real-Time Assisted Writing |
|--------------|--------------------------------------------------------------------------------------------------------|
| Hypothesis question | Does the implementation of a large-scale neural language model and the use of state-of-the-art machine learning techniques for language model training result in a successful system for generating real-time suggestions for reducing repetitive typing in email writing? |
| Key Related Work | 1. Neural network-based language models: The paper highlights the shift from traditional n-gram models to neural network-based language models in NLP and ASR systems. The latter has improved state-of-the-art performance in these systems. <br /> 2. Serving large-scale neural language models: The paper discusses the challenges of serving large-scale neural language models, which are heavy-weight, computationally expensive and stateful, compared to traditional n-gram models that are amenable to large-scale distributed serving. <br /> 3. Smart Compose vs. Smart Reply: The paper contrasts Smart Compose with Google's Smart Reply, a system for generating automated response suggestions for emails. Smart Compose is more flexible and sensitive to context, and generates suggestions for every sentence prefix in real-time, while Smart Reply only generates responses once for each email. <br /> 4. Personalization in language models: The paper mentions that Smart Compose has been extended for personalization, which is a critical feature in language model adaptation techniques. The approach used is a linear interpolation between an n-gram and recurrent neural network language model. <br /> 5. Multilingual models in NLP: The paper notes the importance of multilingual models in NLP, specifically for tasks like machine translation, and mentions that Smart Compose has been extended for multilingual use. The approach used is inspired by using wordpieces and shared models for all languages, which is similar to the work of Johnson et al. in this field. |
| Method |There are three different models are discussed for generating these suggestions: <br />1 .Language Model A (LM-A): This model treats the problem as a language modeling task, where the input to the model is the current email body prefix (the text the user has already written). The contextual information (subject, previous email body, date, time, and locale) is encoded and combined with the input sequence at each time step to better adapt the model to the current email composition environment. <br /> 2.Language Model B (LM-B): In this model, the subject, previous email body, and current email body are combined into a single sequence and used as the input to the language model. This approach is simpler in terms of model structure, but also leads to a much longer sequence length.<br /> 3. Sequence-to-Sequence Model (Seq2Seq): This model formulates the problem as a sequence-to-sequence prediction task similar to neural machine translation. The source sequence is the concatenation of the subject and previous email body (if there is one), and the target sequence is the current email body. Through the attention mechanism, a Seq2Seq model has the potential advantage of better understanding the context information.<br /> The method described is the process of generating email suggestions using the Smart Compose tool. The tool utilizes a language model to predict the next word in an email. At inference time, the necessary contextual information (subject, previous email, etc.) is fed into the model and a beam search algorithm is used to generate n best suggestions. The beam search algorithm works by maintaining a heap of m best candidate sequences. At each step of the beam search, new candidate sequences are generated by extending each candidate sequence by one token and adding them to the heap. The heap is then pruned to keep only the m best candidates. The beam search ends when no new candidate sequences are added or when the candidate sequence reaches a predefined maximum output sequence length. The top suggestion is provided to the user only when the model is confident enough about the suggestion, which is determined by a confidence score based on a length-normalized log conditional probability. The confidence score is used as the value for the ordering within the candidate heap during the beam search.|
| Evluation Matrix |There are 2 evluation matrix: 1. Log Perplexity 2. ExactMatch <br /> Datasets : Previous e-mail in case the composed e-mail was a response, Subject of the e-mail, Date and time of the composed e-mail, Locale of the user composing the e-mail. <br />
|  Results | 1. for LM-A : Transformer-1536-8192 has lower Log Perplexity Here,  1536 is the layer and 8192 is the hidden size and Transformer 1536-8192 has the better in extact match.   <br /> 2. LM-B Transformer-2048-8192 has lower Test Log Perplexity.<br /> 3. for Results using the Seq2Seq approach BiLSTM-6-LSTM-8-1024 has lower Log Perplexity.
| Conclusion | The Smart Compose system aims to improve the writing experience of Gmail users by providing real-time, context-dependent, and diverse suggestions as they type. The system has been designed to overcome challenges such as model design, evaluation, serving, privacy, and fairness, and has been enhanced with features such as personalization and multilingual support. The authors of the paper have conducted extensive experiments comparing the performance of different model and serving architectures and have demonstrated that the Smart Compose system is effective in improving the writing experience for Gmail users.|


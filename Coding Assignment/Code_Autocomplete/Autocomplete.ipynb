{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881,
          "referenced_widgets": [
            "eada72246df249cc836e6f7950b9acea",
            "434a895a084f48cd9d09a88136f1add0",
            "300f113a0c2e4b75991ecf9c356e911b",
            "f16925a5f5df475b904f92cbd9ba7d4f",
            "eaf0f0cbc6614b659e0306834514831d",
            "adc0d7629e2441cb967a7ba92079b924",
            "1ef0545dcded4fa4ba3e337bc9af230a",
            "1ed4f6676e3142aab4b01da08648a3c5",
            "79caa20f641e42a0bbd20cd291ace408",
            "be4d4c0561664c63ac6da470f919ab26",
            "9407eeac23a1442493e2a9dd6c41bdb9"
          ]
        },
        "id": "wuXRHt1CF-QY",
        "outputId": "62741adc-ad78-4284-b83e-9aad98e27205"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Sirikit Joshi\\.conda\\envs\\cudaenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Using custom data configuration codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1\n",
            "Found cached dataset parquet (C:/Users/Sirikit Joshi/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|██████████| 2/2 [00:00<00:00,  5.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['repo_name', 'path', 'license', 'content'],\n",
            "        num_rows: 47452\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['repo_name', 'path', 'license', 'content'],\n",
            "        num_rows: 11864\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# !pip install datasets\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext, datasets, math\n",
        "from tqdm import tqdm\n",
        "dataset = datasets.load_dataset(\"codeparrot/github-jupyter-code-to-text\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gsq3n7PaHSoK",
        "outputId": "039909b1-251b-403e-ba93-5ec061b2fb65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['repo_name', 'path', 'license', 'content'])\n"
          ]
        }
      ],
      "source": [
        "print(dataset['train'][333].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnCbhr6J0zrv",
        "outputId": "52e28987-4dc1-462d-c95f-fe9c001e3947"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11367363 2875424\n"
          ]
        }
      ],
      "source": [
        "total_train_dataset =[]\n",
        "total_test_dataset = []\n",
        "for text in dataset['train']['content']:\n",
        "  for split in text.split('\\n'):\n",
        "    if split != \"\":\n",
        "      total_train_dataset.append(split)\n",
        "\n",
        "for text in dataset['test']['content']:\n",
        "  for split in text.split('\\n'):\n",
        "    if split != \"\":\n",
        "      total_test_dataset.append(split)\n",
        "\n",
        "print(len(total_train_dataset),len(total_test_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LuR3rLIbGpdi"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "tokenized_dataset_train = yield_tokens(total_train_dataset[:int(len(total_train_dataset)/100)])\n",
        "tokenized_dataset_test = yield_tokens(total_test_dataset[:int(len(total_test_dataset)/100)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0-jiOrm20Hn",
        "outputId": "0a92484a-657e-47e5-a0ae-02dabd94c083"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "# !python3 -m spacy download en_core_web_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UQeQYVNG0zCJ"
      },
      "outputs": [],
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import spacy\n",
        "import re\n",
        "import spacy\n",
        "#spacy.load(\"en_core_web_s\")\n",
        "# import en_core_web_md\n",
        "# nlp = en_core_web_md.load()\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "def preprocessing(sentence):\n",
        "    \n",
        "    # Clear the html tag by using regular expression.\n",
        "    sentence = re.sub(\"<[^>]*>\", \"\", sentence)\n",
        "    sentence = re.sub(\"[^\\x00-\\x7F]+\", \"\", sentence) #extract non-english out\n",
        "    #It matches any character which is not contained in the ASCII character set (0-127, i.e. 0x0 to 0x7F)\n",
        "    stopwords = list(STOP_WORDS)\n",
        "    doc = nlp(sentence)\n",
        "    cleaned_tokens = []\n",
        "    \n",
        "    for token in doc:\n",
        "        if token.text not in stopwords and token.pos_ != 'PUNCT' and token.pos_ != 'SPACE' and \\\n",
        "            token.pos_ != 'SYM' and token.pos_!= 'X':\n",
        "                cleaned_tokens.append(token.lemma_.lower().strip())\n",
        "                \n",
        "    return \" \".join(cleaned_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAOeO_GL0gcJ",
        "outputId": "ebb27e51-e6aa-4f99-e7de-7591419aef24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<built-in function len>\n"
          ]
        }
      ],
      "source": [
        "print(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI90bUMvJV_Z",
        "outputId": "bf982536-6002-40a6-c5d2-65001012b87c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab Size 2769\n",
            "['<unk>', '<eos>', '=', 'explanation', '#', 'end', \"'\", 'import', 'datum', 'property']\n"
          ]
        }
      ],
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(preprocessing(text))\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(total_train_dataset[:int(len(total_test_dataset)/100)]), min_freq=5) \n",
        "vocab.insert_token('<unk>', 0)           \n",
        "vocab.insert_token('<eos>', 1)            \n",
        "vocab.set_default_index(vocab['<unk>'])   \n",
        "print('Vocab Size',len(vocab))                         \n",
        "print(vocab.get_itos()[:10]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zSu5e5iTJarU"
      },
      "outputs": [],
      "source": [
        "def get_data(dataset, vocab, batch_size):\n",
        "    data = []                                                   \n",
        "    for example in dataset:       \n",
        "        #appends eos so we know it ends....so model learn how to end...                             \n",
        "        tokens = example.append('<eos>') #end of sentence\n",
        "        #numericalize          \n",
        "        tokens = [vocab[token] for token in example] \n",
        "        data.extend(tokens)                                    \n",
        "    data = torch.LongTensor(data)                                 \n",
        "    num_batches = data.shape[0] // batch_size \n",
        "    data = data[:num_batches * batch_size]                       \n",
        "    data = data.view(batch_size, num_batches)        \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sg-KGOKZIS9f"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "train_data = get_data(tokenized_dataset_train, vocab, batch_size)\n",
        "valid_data = get_data(tokenized_dataset_test, vocab, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBZ5acU_66JT",
        "outputId": "eef31b4d-3cc6-44bf-c254-f442fd0308d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([128, 8296]), torch.Size([128, 2204]))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.shape, valid_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CfKdO3iQR4K6"
      },
      "outputs": [],
      "source": [
        "class LSTMLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
        "                \n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hid_dim = hid_dim\n",
        "        self.emb_dim = emb_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, \n",
        "                    dropout=dropout_rate, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
        "        \n",
        "        self.init_weights()\n",
        "        \n",
        "    def init_weights(self):\n",
        "        init_range_emb = 0.1\n",
        "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
        "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
        "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
        "        self.fc.bias.data.zero_()\n",
        "        for i in range(self.num_layers):\n",
        "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
        "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
        "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim, \n",
        "                    self.hid_dim).uniform_(-init_range_other, init_range_other) \n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
        "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
        "        return hidden, cell\n",
        "    \n",
        "    def detach_hidden(self, hidden):\n",
        "        hidden, cell = hidden\n",
        "        hidden = hidden.detach()\n",
        "        cell = cell.detach()\n",
        "        return hidden, cell\n",
        "\n",
        "    def forward(self, src, hidden):\n",
        "        #src: [batch size, seq len]\n",
        "        embedding = self.dropout(self.embedding(src))\n",
        "        #embedding: [batch size, seq len, emb_dim]\n",
        "        output, hidden = self.lstm(embedding, hidden)      \n",
        "        #output: [batch size, seq len, hid_dim]\n",
        "        #hidden = h, c = [num_layers * direction, seq len, hid_dim)\n",
        "        output = self.dropout(output) \n",
        "        prediction = self.fc(output)\n",
        "        #prediction: [batch size, seq_len, vocab size]\n",
        "        return prediction, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4jEWBOPGR8VI"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "emb_dim = 1024                # 400 in the paper\n",
        "hid_dim = 1024                # 1150 in the paper\n",
        "num_layers = 2                # 3 in the paper\n",
        "dropout_rate = 0.65              \n",
        "lr = 1e-3  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5BTl2XQgSB5c"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL3_03WBR_hq",
        "outputId": "c0c59a37-cc3e-4b20-b4d8-a8b7540284bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 22,467,281 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {num_params:,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hf0PvcpaSHY8"
      },
      "outputs": [],
      "source": [
        "def get_batch(data, seq_len, idx):\n",
        "    #data #[batch size, bunch of tokens]\n",
        "    src    = data[:, idx:idx+seq_len]                   \n",
        "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
        "    return src, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4ZHh1rlNSTeq"
      },
      "outputs": [],
      "source": [
        "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    # drop all batches that are not a multiple of seq_len\n",
        "    # data #[batch size, bunch of tokens]\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
        "    num_batches = data.shape[-1]\n",
        "    \n",
        "    #reset the hidden every epoch\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    \n",
        "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #hidden does not need to be in the computational graph for efficiency\n",
        "        hidden = model.detach_hidden(hidden)\n",
        "\n",
        "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
        "        src, target = src.to(device), target.to(device)\n",
        "        batch_size = src.shape[0]\n",
        "        prediction, hidden = model(src, hidden)               \n",
        "\n",
        "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
        "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
        "        target = target.reshape(-1)\n",
        "        loss = criterion(prediction, target)\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "s2EKrpqtSeSw"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.eval()\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in range(0, num_batches - 1, seq_len):\n",
        "            hidden = model.detach_hidden(hidden)\n",
        "            src, target = get_batch(data, seq_len, idx)\n",
        "            src, target = src.to(device), target.to(device)\n",
        "            batch_size= src.shape[0]\n",
        "\n",
        "            prediction, hidden = model(src, hidden)\n",
        "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
        "            target = target.reshape(-1)\n",
        "\n",
        "            loss = criterion(prediction, target)\n",
        "            epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XO4TzoDrSic-",
        "outputId": "4f5269fc-e05d-4b28-9460-ddf03cd6e212"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 1\n",
            "\tTrain Perplexity: 17.813\n",
            "\tValid Perplexity: 11.747\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 2\n",
            "\tTrain Perplexity: 9.435\n",
            "\tValid Perplexity: 9.807\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 3\n",
            "\tTrain Perplexity: 8.293\n",
            "\tValid Perplexity: 9.153\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 4\n",
            "\tTrain Perplexity: 7.696\n",
            "\tValid Perplexity: 8.783\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 5\n",
            "\tTrain Perplexity: 7.272\n",
            "\tValid Perplexity: 8.560\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 6\n",
            "\tTrain Perplexity: 6.941\n",
            "\tValid Perplexity: 8.360\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 7\n",
            "\tTrain Perplexity: 6.664\n",
            "\tValid Perplexity: 8.236\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 8\n",
            "\tTrain Perplexity: 6.431\n",
            "\tValid Perplexity: 8.119\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 9\n",
            "\tTrain Perplexity: 6.219\n",
            "\tValid Perplexity: 8.046\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 10\n",
            "\tTrain Perplexity: 6.030\n",
            "\tValid Perplexity: 7.979\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 11\n",
            "\tTrain Perplexity: 5.864\n",
            "\tValid Perplexity: 7.914\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 12\n",
            "\tTrain Perplexity: 5.711\n",
            "\tValid Perplexity: 7.854\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 13\n",
            "\tTrain Perplexity: 5.574\n",
            "\tValid Perplexity: 7.844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 14\n",
            "\tTrain Perplexity: 5.454\n",
            "\tValid Perplexity: 7.849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 15\n",
            "\tTrain Perplexity: 5.244\n",
            "\tValid Perplexity: 7.810\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 16\n",
            "\tTrain Perplexity: 5.124\n",
            "\tValid Perplexity: 7.808\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 17\n",
            "\tTrain Perplexity: 5.040\n",
            "\tValid Perplexity: 7.789\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 18\n",
            "\tTrain Perplexity: 4.958\n",
            "\tValid Perplexity: 7.786\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 19\n",
            "\tTrain Perplexity: 4.892\n",
            "\tValid Perplexity: 7.798\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tepoch: 20\n",
            "\tTrain Perplexity: 4.793\n",
            "\tValid Perplexity: 7.765\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 20\n",
        "seq_len  = 30 #<----decoding length\n",
        "clip    = 0.25\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = train(model, train_data, optimizer, criterion, \n",
        "                batch_size, seq_len, clip, device)\n",
        "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
        "                seq_len, device)\n",
        "\n",
        "    lr_scheduler.step(valid_loss)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model/best-val-auto.pt')\n",
        "    print(f'\\tepoch: {epoch+1}')\n",
        "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
        "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1wr2p_ZkSvQr"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    model.eval()\n",
        "    tokens = tokenizer(prompt)\n",
        "    indices = [vocab[t] for t in tokens]\n",
        "    batch_size = 1\n",
        "    hidden = model.init_hidden(batch_size, device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(max_seq_len):\n",
        "            src = torch.LongTensor([indices]).to(device)\n",
        "            prediction, hidden = model(src, hidden)\n",
        "            \n",
        "            #prediction: [batch size, seq len, vocab size]\n",
        "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
        "            \n",
        "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
        "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
        "            \n",
        "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
        "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
        "                break\n",
        "\n",
        "            indices.append(prediction) #autoregressive, thus output becomes input\n",
        "\n",
        "    itos = vocab.get_itos()\n",
        "    tokens = [itos[i] for i in indices]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UMw9g_AA7Nn",
        "outputId": "2f91412f-8bbc-4acc-f06e-14b8147cb445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5\n",
            "import numpy as np\n",
            "\n",
            "0.7\n",
            "import numpy as np\n",
            "\n",
            "0.75\n",
            "import numpy as np\n",
            "\n",
            "0.8\n",
            "import numpy as np\n",
            "\n",
            "1.0\n",
            "import numpy as np\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = 'import numpy'\n",
        "max_seq_len = 30\n",
        "seed = 0\n",
        "            #superdiverse   more diverse\n",
        "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0] \n",
        "#sample from this distribution higher probability will get more change\n",
        "for temperature in temperatures:\n",
        "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
        "                          vocab, device, seed)\n",
        "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Nj0-DUVA8Pu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "cudaenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "baa027ae8c7a15e707847f8b27a7cc0bd2f7ea77c3d15ba3fcbc1c2cc6af2370"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ed4f6676e3142aab4b01da08648a3c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ef0545dcded4fa4ba3e337bc9af230a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "300f113a0c2e4b75991ecf9c356e911b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ed4f6676e3142aab4b01da08648a3c5",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79caa20f641e42a0bbd20cd291ace408",
            "value": 2
          }
        },
        "434a895a084f48cd9d09a88136f1add0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adc0d7629e2441cb967a7ba92079b924",
            "placeholder": "​",
            "style": "IPY_MODEL_1ef0545dcded4fa4ba3e337bc9af230a",
            "value": "100%"
          }
        },
        "79caa20f641e42a0bbd20cd291ace408": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9407eeac23a1442493e2a9dd6c41bdb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adc0d7629e2441cb967a7ba92079b924": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be4d4c0561664c63ac6da470f919ab26": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eada72246df249cc836e6f7950b9acea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_434a895a084f48cd9d09a88136f1add0",
              "IPY_MODEL_300f113a0c2e4b75991ecf9c356e911b",
              "IPY_MODEL_f16925a5f5df475b904f92cbd9ba7d4f"
            ],
            "layout": "IPY_MODEL_eaf0f0cbc6614b659e0306834514831d"
          }
        },
        "eaf0f0cbc6614b659e0306834514831d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f16925a5f5df475b904f92cbd9ba7d4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be4d4c0561664c63ac6da470f919ab26",
            "placeholder": "​",
            "style": "IPY_MODEL_9407eeac23a1442493e2a9dd6c41bdb9",
            "value": " 2/2 [00:00&lt;00:00, 38.36it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

## A Fast and Accurate Dependency Parser using Neural Networks

### Authors: Danqi Chen and Christopher D. Manning

| Topic  | A Fast and Accurate Dependency Parser using Neural Networks |
|--------------|--------------------------------------------------------------------------------------------------------|
| Hypothesis question | Dependency Parser cannot handle complex real-world sentences in a fast and accurate manner |
| Key Related Work | 1. Neural network-based language models: The paper highlights the shift from traditional n-gram models to neural network-based language models in NLP and ASR systems. The latter has improved state-of-the-art performance in these systems. <br /> 2. |
| Method |There are three different models are discussed for generating these suggestions: <br />1 .Language Model A (LM-A) <br /> 2.|
|  Results | 1. for LM-A : Transformer-1536-8192 has lower Log Perplexity Here,  1536 is the layer and 8192 is the hidden size and Transformer 1536-8192 has the better in extact match.   <br /> 2. LM-B Transformer-2048-8192 has lower Test Log Perplexity.|
| Conclusion | conclusion here|
| Limitation and Future work| something here |


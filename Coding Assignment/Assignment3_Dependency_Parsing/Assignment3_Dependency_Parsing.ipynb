{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Dependency Parsing Homework\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment:**\n",
    "\n",
    "1. Read https://aclanthology.org/D14-1082.pdf and maybe just write one paragraph summary in your README.md in your github\n",
    "\n",
    "2. Do something called ablation study (meaning try to delete something so we know the impact of that deleted thing - very common in NLP)\n",
    "Recall that we have 18 word + 18 pos + 12 dep features\n",
    "Try to delete only the 12 dep features and check UAS\n",
    "Try to delete only the 18 pos features and check UAS\n",
    "3. Do another comparison study testing the embedding\n",
    "Chaky uses some embedding\n",
    "Try to use (1) glove embedding (smallest), (2) nn.Embedding (train from scratch) and compare with Chaky's embedding - on how it affects the UAS\n",
    "4. Do some testing, compare 2-3 sentences with spaCy and see whether our neural network gives the same dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm  \n",
    "import pickle \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parsing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's make a class `Parsing`, representing a parser for each sentence.  For each sentence, we need the `stack`, `buffer`, and the `dependencies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parsing(object):\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence #['The', 'cat', 'sat']\n",
    "        \n",
    "        # The current stack represented as a list with the top of the stack as the\n",
    "        # last element of the list.\n",
    "        self.stack = ['ROOT']\n",
    "        \n",
    "        # The current buffer represented as a list with the first item on the\n",
    "        # buffer as the first item of the list\n",
    "        self.buffer = sentence[:]\n",
    "        \n",
    "        # The list of dependencies produced so far. Represented as a list of\n",
    "        # tuples where each tuple is of the form (head, dependent).\n",
    "        self.dep = []\n",
    "    \n",
    "    #Let's first create a function to perform \"Shift\", \"Left-Arc\", and \"Right-Arc\"\n",
    "    def parse_step(self, transition):\n",
    "        if transition == 'S':\n",
    "            buffer_head = self.buffer.pop(0)\n",
    "            self.stack.append(buffer_head)\n",
    "        #stack: [ROOT, I, parsed]\n",
    "        elif transition == 'LA':\n",
    "            dependent = self.stack.pop(-2) #I\n",
    "            self.dep.append((self.stack[-1], dependent)) #(parsed, I)\n",
    "        #stack: [ROOT, parsed, sentence]\n",
    "        elif transition == 'RA':\n",
    "            dependent = self.stack.pop() #sentence\n",
    "            self.dep.append((self.stack[-1], dependent)) #(parsed, sentence)\n",
    "        else:\n",
    "            print(f\"Unknown transition: {transition}\")\n",
    "            \n",
    "    #loop all transitions\n",
    "    def parse(self, transitions):\n",
    "        for transition in transitions:\n",
    "            self.parse_step(transition)\n",
    "        return self.dep\n",
    "    \n",
    "    #an utility function to check that the parsing is done.\n",
    "    def is_completed(self):\n",
    "        return (len(self.buffer) == 0) and (len(self.stack) == 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the parse step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['The'], ['sat'], [('The', 'ROOT'), ('The', 'cat')])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsing = Parsing([\"The\", \"cat\", \"sat\"])\n",
    "parsing.parse([\"S\", \"LA\", \"S\", \"RA\"])\n",
    "parsing.stack, parsing.buffer, parsing.dep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch parsing\n",
    "\n",
    "Let's create a minibatch loader that loads a bunch of sentences, and perform parse accordingly.  For now, we will assume a very dump model to predict the transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_parse(sentences, model, batch_size):\n",
    "    dep = []\n",
    "\n",
    "    # Initialize a list of DepParser, one for each sentence.\n",
    "    partial_parses = [Parsing(sentence) for sentence in sentences]\n",
    "    \n",
    "    # Shallow copy partial_parses.\n",
    "    unfinished_parses = partial_parses[:]\n",
    "        \n",
    "    # While unfinished_parses is not empty do.\n",
    "    while unfinished_parses:\n",
    "        # Take first batch_size parses in unfinished_parses as minibatch.\n",
    "        minibatch = unfinished_parses[:batch_size]\n",
    "        # Use model to predict next transition for each partial parse in the minibatch.\n",
    "        transitions = model.predict(minibatch)\n",
    "        # Perform parse step for each partial_parse_minibatch with their predicted transition.\n",
    "        for transition, partial_parse in zip(transitions, minibatch):\n",
    "            partial_parse.parse_step(transition)\n",
    "        # Remove completed parses.\n",
    "        unfinished_parses[:] = [\n",
    "            p for p in unfinished_parses if not p.is_completed()]\n",
    "    dep = [parse.dep for parse in partial_parses]\n",
    "\n",
    "    return dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(object):\n",
    "    \"\"\"\n",
    "    Dummy model for testing the minibatch_parse function\n",
    "    First shifts everything onto the stack and then does exclusively right arcs if the first word of\n",
    "    the sentence is \"right\", \"left\" if otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    def predict(self, partial_parses):\n",
    "        return [(\"RA\" if pp.stack[1] == \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
    "                for pp in partial_parses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [[\"right\", \"arcs\", \"only\"],\n",
    "             [\"right\", \"arcs\", \"only\", \"again\"],\n",
    "             [\"left\", \"arcs\", \"only\"],\n",
    "             [\"left\", \"arcs\", \"only\", \"again\"]]\n",
    "deps = minibatch_parse(sentences, DummyModel(), 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We used English Penn Treebank dataset in CoNLL format.\n",
    "\n",
    "CoNLL is the conventional name for TSV formats in NLP (TSV - tab-separated values, i.e., CSV with <TAB> as separator).\n",
    "It originates from a series of shared tasks organized at the Conferences of Natural Language Learning (hence the name)\n",
    "\n",
    "In CoNLL formats,\n",
    "- every word (token) is represented in one line\n",
    "- every sentence is separated from the next by an empty line\n",
    "- every column represents one annotation\n",
    "\n",
    "There are many formats, in our case, our conll file has 10 columns, the important columns are:\n",
    "- 1:  word\n",
    "- 4:  pos\n",
    "- 6:  head of the dependency\n",
    "- 7:  type of dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for reading the conll file\n",
    "def read_conll(filename, max_example=5):\n",
    "    examples = []\n",
    "    with open(filename) as f:\n",
    "        i = 0\n",
    "        word, pos, head, dep = [], [], [], []\n",
    "        for line in f.readlines():\n",
    "            i = i + 1\n",
    "            wa = line.strip().split('\\t') #wa = word annotations\n",
    "            \n",
    "            #if all 10 columns are there....\n",
    "            if len(wa) == 10:  \n",
    "                word.append(wa[1].lower())\n",
    "                pos.append(wa[4])\n",
    "                head.append(int(wa[6]))\n",
    "                dep.append(wa[7])\n",
    "            \n",
    "            #otherwise, it means the line is empty, thus we consider finish for one example\n",
    "            elif len(word) > 0:\n",
    "                examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})\n",
    "                word, pos, head, dep = [], [], [], []\n",
    "                    \n",
    "        #last example\n",
    "        if len(word) > 0:\n",
    "            examples.append({'word': word, 'pos': pos, 'head': head, 'dep': dep})\n",
    "            \n",
    "    return examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the load function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print(\"1. Loading data\")\n",
    "    train_set = read_conll(\"train.conll\")\n",
    "    dev_set = read_conll(\"dev.conll\")\n",
    "    test_set = read_conll(\"test.conll\")\n",
    "    \n",
    "    return train_set[:1000], dev_set[:500], test_set[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data\n"
     ]
    }
   ],
   "source": [
    "train_set, dev_set, test_set = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 500, 500)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the size of datasets\n",
    "len(train_set), len(dev_set), len(test_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parser"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a parser that will help us:\n",
    "- create a `tok2id` dictionary in the `__init__` function\n",
    "- numercalize `numercalize` the words, dependencies, and pos tags\n",
    "- create training data, `create_instances` by leveraging the ground truth of the dependencies\n",
    "- finally the `parse` function\n",
    "\n",
    "This feature vector consists of a list of tokens. They can be represented as a list of integers $\\mathbf{w} = [w_1, w_2, \\cdots, w_m]$ where $m$ is the number of features and each $0 \\leq w_i \\leq |V|$ is the index of a token in the vocabulary ($|V|$ is the vocabulary size).  Then our network looks up an embedding for each word and concatenates them into a single input vector:\n",
    "\n",
    "$$\\mathbf{x} = [\\mathbf{E}_{w_1}, \\cdots, \\mathbf{E}_{w_m}] \\in \\mathbb{R}^{dm}$$\n",
    "\n",
    "where $\\mathbf{E} \\in \\mathbb{R}^{|V| \\times d}$ is an embedding matrix with each row $\\mathbf{E}_w$ as the vector for a particular word $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_PREFIX = '<p>:' #indicating pos tags\n",
    "D_PREFIX = '<d>:' #indicating dependency tags\n",
    "UNK      = '<UNK>'\n",
    "NULL     = '<NULL>'\n",
    "ROOT     = '<ROOT>'\n",
    "\n",
    "class Parser(object):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        #set the root dep\n",
    "        self.root_dep = 'root'\n",
    "                \n",
    "        #get all the dep of the dataset as list, e.g., ['root', 'acl', 'nmod', 'nmod:npmod']\n",
    "        all_dep = [self.root_dep] + list(set([w for ex in dataset\n",
    "                                               for w in ex['dep']\n",
    "                                               if w != self.root_dep]))\n",
    "        \n",
    "        #1. put dep into tok2id lookup table, with D_PREFIX so we know it is dependency\n",
    "        #{'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'D_PREFIX:<NULL>': 30}\n",
    "        tok2id = {D_PREFIX + l: i for (i, l) in enumerate(all_dep)}\n",
    "        tok2id[D_PREFIX + NULL] = self.D_NULL = len(tok2id)\n",
    "        \n",
    "        #we are using \"unlabeled\" where we do not label with the dependency\n",
    "        #thus the number of dependency relation is 1\n",
    "        trans = ['L', 'R', 'S']\n",
    "        self.n_deprel = 1\n",
    "        \n",
    "        #create a simple lookup table mapping action and id\n",
    "        #e.g., tran2id: {'L': 0, 'R': 1, 'S': 2}\n",
    "        #e.g., id2tran: {0: 'L', 1: 'R', 2: 'S'}\n",
    "        self.n_trans = len(trans)\n",
    "        self.tran2id = {t: i for (i, t) in enumerate(trans)}\n",
    "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
    "\n",
    "        #2. put pos tags into tok2id lookup table, with P_PREFIX so we know it is pos\n",
    "        tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[P_PREFIX + UNK]  = self.P_UNK  = len(tok2id)  #also remember the pos tags of unknown\n",
    "        tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
    "        tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
    "        \n",
    "        #now tok2id:  {'P_PREFIX:root': 0, 'P_PREFIX:acl': 1, ..., 'P_PREFIX:JJR': 62, 'P_PREFIX:<UNK>': 63, 'P_PREFIX:<NULL>': 64, 'P_PREFIX:<ROOT>': 65}\n",
    "        \n",
    "        #3. put word into tok2id lookup table\n",
    "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[UNK]  = self.UNK = len(tok2id)\n",
    "        tok2id[NULL] = self.NULL = len(tok2id)\n",
    "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
    "        \n",
    "        #now tok2id: {'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'memory': 340, 'mr.': 341, '<UNK>': 342, '<NULL>': 343, '<ROOT>': 344}\n",
    "        \n",
    "        #create id2tok\n",
    "        self.tok2id = tok2id\n",
    "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
    "        \n",
    "        #why 18 normal features + 18 (pos) + 12 (dep)\n",
    "        #18 features - top 3 words on buffer, top 3 words on stack, \n",
    "        # the first and second left most/rightmost children of the top two words on the stack\n",
    "        # the leftmost of leftmost/rightmost of rightmost children of the top two words on the stack\n",
    "        #18 pos - basically corresponding POS tags\n",
    "        #12 dep - corresponding ARC, excluding 6 words on hte stack/buffer..\n",
    "        self.n_features = 18 + 18 + 12\n",
    "        self.n_tokens = len(tok2id)\n",
    "    \n",
    "    #function to turn train set with words to train set with id instead using tok2id\n",
    "    def numericalize(self, examples):\n",
    "        numer_examples = []\n",
    "        for ex in examples:\n",
    "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
    "                                  else self.UNK for w in ex['word']]\n",
    "            pos  = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
    "                                   else self.P_UNK for w in ex['pos']]\n",
    "            head = [-1] + ex['head']\n",
    "            dep  = [-1] + [self.tok2id[D_PREFIX + w] if D_PREFIX + w in self.tok2id\n",
    "                            else -1 for w in ex['dep']]\n",
    "            numer_examples.append({'word': word, 'pos': pos,\n",
    "                                 'head': head, 'dep': dep})\n",
    "        return numer_examples\n",
    "\n",
    "    #function to extract features to form a feature embedding matrix\n",
    "    def extract_features(self, stack, buf, arcs, ex):\n",
    "             \n",
    "        #ex['word']:  [55, 32, 33, 34, 35, 30], i.e., ['root', 'ms.', 'haag', 'plays', 'elianti', '.']\n",
    "        #ex['pos']:   [29, 14, 14, 16, 14, 17], i.e., ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
    "        #ex['head']:  [-1, 2, 3, 0, 3, 3]  or ['root', 'compound', 'nsubj', 'root', 'dobj', 'punct']}\n",
    "        #ex['dep']:   [-1, 1, 2, 0, 6, 12] or ['compound', 'nsubj', 'root', 'dobj', 'punct']\n",
    "\n",
    "        #stack     :  [0]\n",
    "        #buffer    :  [1, 2, 3, 4, 5]\n",
    "        \n",
    "        if stack[0] == \"ROOT\":\n",
    "            stack[0] = 0  #start the stack with [ROOT]\n",
    "\n",
    "        #get leftmost children based on the dependency arcs\n",
    "        def get_lc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
    "\n",
    "        #get right most children based on the dependency arcs\n",
    "        def get_rc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
    "                          reverse=True)\n",
    "\n",
    "        p_features = [] #pos features (2a, 2b, 2c) - 18\n",
    "        d_features = [] #dep features (3b, 3c) - 12\n",
    "        \n",
    "        #last 3 things on the stack as features\n",
    "        #if the stack is less than 3, then we simply append NULL from the left\n",
    "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
    "        \n",
    "        # next 3 things on the buffer as features\n",
    "        #if the buffer is less than 3, simply append NULL\n",
    "        #the reason why NULL is appended on end because buffer is read left to right\n",
    "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
    "        \n",
    "        #corresponding pos tags\n",
    "        p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
    "        p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
    "        \n",
    "        #get the leftmost and rightmost children of the top two words, thus we loop 2 times\n",
    "        for i in range(2):\n",
    "            if i < len(stack):\n",
    "                k = stack[-i-1] #-1, -2 last two in the stack\n",
    "                \n",
    "                #the first and second lefmost/rightmost children of the top two words (i=1, 2) on the stack\n",
    "                lc = get_lc(k)  \n",
    "                rc = get_rc(k)\n",
    "                \n",
    "                #the leftmost of leftmost/rightmost of rightmost children of the top two words on the stack:\n",
    "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
    "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
    "\n",
    "                #(leftmost of first word on stack, rightmost of first word, \n",
    "                # leftmost of the second word on stack, rightmost of second, \n",
    "                # leftmost of leftmost, rightmost of rightmost\n",
    "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
    "\n",
    "                #corresponding pos\n",
    "                p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
    "                p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
    "            \n",
    "                #corresponding dep\n",
    "                d_features.append(ex['dep'][lc[0]] if len(lc) > 0 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][rc[0]] if len(rc) > 0 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][lc[1]] if len(lc) > 1 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][rc[1]] if len(rc) > 1 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][llc[0]] if len(llc) > 0 else self.D_NULL)\n",
    "                d_features.append(ex['dep'][rrc[0]] if len(rrc) > 0 else self.D_NULL)\n",
    "                \n",
    "            else:\n",
    "                #attach NULL when they don't exist\n",
    "                features += [self.NULL] * 6\n",
    "                p_features += [self.P_NULL] * 6\n",
    "                d_features += [self.D_NULL] * 6\n",
    "\n",
    "        features += p_features + d_features\n",
    "        assert len(features) == self.n_features  #assert they are 18 + 18 + 12\n",
    "        return features\n",
    "\n",
    "    #decide whether to shift, leftarc, or rightarc, based on gold parse trees\n",
    "    #this is needed to create training examples which contain samples and ground truth\n",
    "    def get_oracle(self, stack, buf, ex):\n",
    "\n",
    "        #leave if the stack is only 1, thus nothing to predict....\n",
    "        if len(stack) < 2:\n",
    "            return self.n_trans - 1\n",
    "\n",
    "        #predict based on the last two words on the stack\n",
    "        i0 = stack[-1]\n",
    "        i1 = stack[-2]\n",
    "\n",
    "        #get the head and dependency\n",
    "        h0 = ex['head'][i0]\n",
    "        h1 = ex['head'][i1]\n",
    "        d0 = ex['dep'][i0]\n",
    "        d1 = ex['dep'][i1]\n",
    "\n",
    "        #either shift, left arc or right arc\n",
    "        #\"Shift\" = 2; \"LA\" = 0; \"RA\" = 1\n",
    "        #if head of the second last word is the last word, then leftarc\n",
    "        if (i1 > 0) and (h1 == i0):\n",
    "            return 0\n",
    "        #if head of the last word is the second last word, then rightarc\n",
    "        #make sure nothing in the buffer has head with the last word on the stack\n",
    "        #otherwise, we lose the last word.....\n",
    "        elif (i1 >= 0) and (h0 == i1) and \\\n",
    "                (not any([x for x in buf if ex['head'][x] == i0])):\n",
    "            return 1\n",
    "        #otherwise shift, if something is left in buffer, otherwise, do nothing....\n",
    "        else:\n",
    "            return None if len(buf) == 0 else 2\n",
    "\n",
    "    #generate training examples\n",
    "    #from the training sentences and their gold parse trees \n",
    "    def create_instances(self, examples):\n",
    "        all_instances = []\n",
    "        \n",
    "        for i, ex in enumerate(examples):\n",
    "            #e.g., ex['word]: [344, 163, 99, 164, 165, 68]\n",
    "            n_words = len(ex['word']) - 1  #excluding the root\n",
    "\n",
    "            #arcs = {(head, tail, dependency label)}\n",
    "            stack = [0]\n",
    "            buf = [i + 1 for i in range(n_words)]  #[1, 2, 3, 4, 5]\n",
    "            arcs = []\n",
    "            instances = []\n",
    "            \n",
    "            #because that's the maximum number of shift, leftarcs, rightarcs you can have\n",
    "            #this will determine the sample size of each training example\n",
    "            #if given five words, we will get a sample of (10, 48) where 10 comes from 5 * 2, and 48 is n_features\n",
    "            #but this for loop can be break if there is nothing left....\n",
    "            for i in range(n_words * 2):\n",
    "\n",
    "                #get the gold transition based on the parse trees\n",
    "                #gold_t can be either shift(2), leftarc(0), or rightarc(1)\n",
    "                gold_t = self.get_oracle(stack, buf, ex)\n",
    "                \n",
    "                #if gold_t is None, no need to extract features.....\n",
    "                if gold_t is None:\n",
    "                    break\n",
    "                \n",
    "                #make sure when the model predicts, we inform the current state of stack and buffer, so\n",
    "                #the model is not allowed to make any illegal action, e.g., buffer is empty but trying to pop\n",
    "                legal_labels = self.legal_labels(stack, buf)                \n",
    "                assert legal_labels[gold_t] == 1\n",
    "               \n",
    "                #extract all the 48 features \n",
    "                features = self.extract_features(stack, buf, arcs, ex)\n",
    "                instances.append((features, legal_labels, gold_t))\n",
    "            \n",
    "                #shift \n",
    "                if gold_t == 2:\n",
    "                    stack.append(buf[0])\n",
    "                    buf = buf[1:]\n",
    "                #left arc \n",
    "                elif gold_t == 0:\n",
    "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
    "                    stack = stack[:-2] + [stack[-1]]\n",
    "                #right arc\n",
    "                else:\n",
    "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
    "                    stack = stack[:-1]\n",
    "            else:\n",
    "                all_instances += instances\n",
    "\n",
    "        return all_instances\n",
    "\n",
    "    #provide an one hot encoding of the labels\n",
    "    def legal_labels(self, stack, buf):\n",
    "        labels =  ([1] if len(stack) > 2  else [0]) * self.n_deprel   #left arc   But cannot ROOT <----He thus 3\n",
    "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel   #right arc  ROOT--->He\n",
    "        labels += [1] if len(buf) > 0 else [0]   #shift\n",
    "        return labels\n",
    "    \n",
    "    #a simple function to check punctuation POS tags\n",
    "    def punct(self, pos):\n",
    "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
    "\n",
    "    def parse(self, dataset, eval_batch_size=5000):\n",
    "        sentences = []\n",
    "        sentence_id_to_idx = {}\n",
    "                \n",
    "        for i, example in enumerate(dataset):\n",
    "            \n",
    "            #example['word']=[188, 186, 186, ..., 59]\n",
    "            #n_words=37\n",
    "            #sentence=[1, 2, 3, 4, 5,.., 37]\n",
    "                        \n",
    "            n_words = len(example['word']) - 1\n",
    "            sentence = [j + 1 for j in range(n_words)]            \n",
    "            sentences.append(sentence)\n",
    "            \n",
    "            #mapping the object unique id to the i            \n",
    "            #The id is the object's memory address\n",
    "            sentence_id_to_idx[id(sentence)] = i\n",
    "            \n",
    "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
    "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
    "                \n",
    "        UAS = all_tokens = 0.0\n",
    "        with tqdm(total=len(dataset)) as prog:\n",
    "            for i, ex in enumerate(dataset):\n",
    "                head = [-1] * len(ex['word'])\n",
    "                for h, t, in dependencies[i]:\n",
    "                    head[t] = h\n",
    "                for pred_h, gold_h, gold_l, pos in \\\n",
    "                        zip(head[1:], ex['head'][1:], ex['dep'][1:], ex['pos'][1:]):\n",
    "                        assert self.id2tok[pos].startswith(P_PREFIX)\n",
    "                        pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
    "                        if (not self.punct(pos_str)):\n",
    "                            UAS += 1 if pred_h == gold_h else 0\n",
    "                            all_tokens += 1\n",
    "                prog.update(i + 1)\n",
    "        UAS /= all_tokens\n",
    "        return UAS, dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(object):\n",
    "    def __init__(self, parser, dataset, sentence_id_to_idx):\n",
    "        self.parser = parser\n",
    "        self.dataset = dataset\n",
    "        self.sentence_id_to_idx = sentence_id_to_idx\n",
    "\n",
    "    def predict(self, partial_parses):\n",
    "        mb_x = [self.parser.extract_features(p.stack, p.buffer, p.dep,\n",
    "                                             self.dataset[self.sentence_id_to_idx[id(p.sentence)]])\n",
    "                for p in partial_parses]\n",
    "        mb_x = np.array(mb_x).astype('int32')\n",
    "        mb_x = torch.from_numpy(mb_x).long()\n",
    "        mb_l = [self.parser.legal_labels(p.stack, p.buffer) for p in partial_parses]\n",
    "\n",
    "        pred = self.parser.model(mb_x)\n",
    "        pred = pred.detach().numpy()\n",
    "        \n",
    "        #we need to multiply 10000 with legal labels, to force the model not to make any impossible prediction\n",
    "        #other, when we parse sequentially, sometimes there is nothing in the buffer or stack, thus error....        \n",
    "        pred = np.argmax(pred + 10000 * np.array(mb_l).astype('float32'), 1)\n",
    "        pred = [\"S\" if p == 2 else (\"LA\" if p == 0 else \"RA\") for p in pred]\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a simple function to create ids.....\n",
    "def build_dict(keys, offset=0):\n",
    "    #keys = ['P_PREFIX:IN', 'P_PREFIX:DT', 'P_PREFIX:NNP', 'P_PREFIX:CD', so on...]\n",
    "    #offset is needed because this tok2id has something already inside....\n",
    "    count = Counter()\n",
    "    for key in keys:\n",
    "        count[key] += 1\n",
    "    \n",
    "    #most_common = [('P_PREFIX:NN', 70), ('P_PREFIX:IN', 57), ... , ('P_PREFIX:JJR', 1)]\n",
    "    #we use most_common in case we only want some maximum pos tags....\n",
    "    mc = count.most_common()\n",
    "    \n",
    "    #{'P_PREFIX:NN': 31, 'P_PREFIX:IN': 32, .., 'P_PREFIX:JJR': 62} \n",
    "    return {w[0]: index + offset for (index, w) in enumerate(mc)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the parser `__init__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Building parser...\n",
      "took 0.02 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"2. Building parser...\",)\n",
    "start = time.time()\n",
    "parser = Parser(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the `numericalize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  ['ms.', 'haag', 'plays', 'elianti', '.']\n",
      "Pos:  ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
      "Head:  [2, 3, 0, 3, 3]\n",
      "Dep:  ['compound', 'nsubj', 'root', 'dobj', 'punct']\n"
     ]
    }
   ],
   "source": [
    "print(\"Word: \", train_set[1][\"word\"])\n",
    "print(\"Pos: \",  train_set[1][\"pos\"])\n",
    "print(\"Head: \", train_set[1][\"head\"])\n",
    "print(\"Dep: \",  train_set[1][\"dep\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Numericalizing data...\n",
      "took 0.05 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"3. Numericalizing data...\",)\n",
    "start = time.time()\n",
    "train_set = parser.numericalize(train_set)\n",
    "dev_set   = parser.numericalize(dev_set)\n",
    "test_set  = parser.numericalize(test_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  [5156, 304, 1364, 1002, 2144, 87]\n",
      "Pos:  [84, 42, 42, 55, 42, 46]\n",
      "Head:  [-1, 2, 3, 0, 3, 3]\n",
      "Dep:  [-1, 3, 18, 0, 23, 26]\n"
     ]
    }
   ],
   "source": [
    "print(\"Word: \", train_set[1][\"word\"])\n",
    "print(\"Pos: \",  train_set[1][\"pos\"])\n",
    "print(\"Head: \", train_set[1][\"head\"])\n",
    "print(\"Dep: \",  train_set[1][\"dep\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding length of 50.  In the paper, they applied a custom 50-embedding to all the words, pos, and dependencies.  For pos and dependencies, they claimed that there are some similarities that can be learned as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Loading pretrained embeddings...\n",
      "Embedding matrix shape (vocab, emb size):  (5157, 50)\n",
      "took 5.36 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"4. Loading pretrained embeddings...\",)\n",
    "start = time.time()\n",
    "word_vectors = {}\n",
    "for line in open(\"en-cw.txt\").readlines():\n",
    "    we = line.strip().split() #we = word embeddings - first column: word;  the rest is embedding\n",
    "    word_vectors[we[0]] = [float(x) for x in we[1:]] #{word: [list of 50 numbers], nextword: [another list], so on...}\n",
    "    \n",
    "#create an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
    "#we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token]\n",
    "        elif token.lower() in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Preprocessing training data...\n",
      "took 2.10 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"5. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser.create_instances(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Minibatch loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(data, minibatch_size, shuffle=True):\n",
    "    data_size = len(data[0])\n",
    "    indices = np.arange(data_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
    "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
    "        yield [_minibatch(d, minibatch_indices) for d in data]\n",
    "\n",
    "def _minibatch(data, minibatch_idx):\n",
    "    return data[minibatch_idx] if type(data) is np.ndarray else [data[i] for i in minibatch_idx]\n",
    "\n",
    "def minibatches(data, batch_size):\n",
    "    x = np.array([d[0] for d in data])\n",
    "    y = np.array([d[2] for d in data])\n",
    "    one_hot = np.zeros((y.size, 3))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return get_minibatches([x, one_hot], batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing your minibatch loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (train_x, train_y) in enumerate(minibatches(train_examples, 1024)):\n",
    "#     print(train_x.shape)  #batch size, features\n",
    "#     print(train_y.shape)        #one hot encoding of 3 actions - shift, la, ra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a neural network to predict, given the state of the stack, buffer, and dependencies, which transition should be applied next.\n",
    "\n",
    "Recall that our input vector is:\n",
    "\n",
    "$$\\mathbf{x} = [\\mathbf{E}_{w_1}, \\cdots, \\mathbf{E}_{w_m}] \\in \\mathbb{R}^{dm}$$\n",
    "\n",
    "where $\\mathbf{E} \\in \\mathbb{R}^{|V| \\times d}$ is an embedding matrix with each row $\\mathbf{E}_w$ as the vector for a particular word $w$\n",
    "\n",
    "We then compute our prediction as:\n",
    "\n",
    "$$\\mathbf{h} = \\text{ReLU}(\\mathbf{xW} + \\mathbf{b}_1)$$\n",
    "$$\\mathbf{l} = \\mathbf{hU} + \\mathbf{b}_2$$\n",
    "$$\\hat{\\mathbf{y}} = \\text{softmax}(l)$$\n",
    "\n",
    "where $\\mathbf{h}$ is referred to as the hidden layer, $\\mathbf{l}$ is the logits, $\\hat{\\mathbf{y}}$ is the predictions, and $\\text{ReLU}(z) = \\text{max}(z, 0))$.  We will then train the model to minimize cross-entropy (CE) loss:\n",
    "\n",
    "$$J(\\theta) = \\text{CE}(\\mathbf{y}, \\hat{\\mathbf{y}}) = -\\sum_{i=1}^{3}y_i \\log \\hat{y}_i$$\n",
    "\n",
    "To compute the loss for the training set, we average this $J(\\theta)$ across all training examples.  We will use UAS (Unlabeled Attachment Score) as main metric, which is computed as the ratio between number of correctly predicted dependencies and the number of total dependencies despite of the relations (which our model doesn't predict the relation since is unlabeled). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParserModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings, n_features=48,\n",
    "                 hidden_size=400, n_classes=3, dropout_prob=0.5):\n",
    "\n",
    "        super(ParserModel, self).__init__()\n",
    "        self.n_features   = n_features\n",
    "        self.n_classes    = n_classes\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.embed_size   = embeddings.shape[1]\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
    "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "\n",
    "        self.embed_to_hidden = nn.Linear(self.n_features * self.embed_size, hidden_size)\n",
    "        nn.init.xavier_uniform_(self.embed_to_hidden.weight, gain=1.)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        self.hidden_to_logits = nn.Linear(self.hidden_size, self.n_classes)\n",
    "        nn.init.xavier_uniform_(self.hidden_to_logits.weight)\n",
    "\n",
    "    def embedding_lookup(self, t):\n",
    "        #t:  batch_size, n_features\n",
    "        batch_size = t.size()[0]\n",
    "                    \n",
    "        x = self.pretrained_embeddings(t)        \n",
    "        x = x.reshape(-1, self.n_features * self.embed_size)\n",
    "        # x = (1024, 48 * 50)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, t):\n",
    "        # t: (1024, 48)\n",
    "        embeddings = self.embedding_lookup(t)  \n",
    "    \n",
    "        # embeddings: (1024, 48 * 50)\n",
    "        hidden = self.embed_to_hidden(embeddings)\n",
    "    \n",
    "        # hidden: (1024, 200)\n",
    "        hidden_activations = F.relu(hidden)\n",
    "        # hidden_activations: (1024, 200)\n",
    "        thin_net = self.dropout(hidden_activations)\n",
    "        # thin_net: (1024, 200)\n",
    "        logits = self.hidden_to_logits(thin_net)\n",
    "        # logits: (1024, 3)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ParserModel(embeddings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParserModel(\n",
       "  (pretrained_embeddings): Embedding(5157, 50)\n",
       "  (embed_to_hidden): Linear(in_features=2400, out_features=400, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (hidden_to_logits): Linear(in_features=400, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now complete the <code>train_for_epoch</code> and <code>train</code> functions to actually train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a class to get the average.....\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
    "    \n",
    "    best_dev_UAS = 0\n",
    "    \n",
    "    optimizer = optim.Adam(parser.model.parameters(), lr=0.001)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
    "        dev_UAS = train_for_epoch(\n",
    "            parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
    "        if dev_UAS > best_dev_UAS:\n",
    "            best_dev_UAS = dev_UAS\n",
    "            print(\"New best dev UAS! Saving model.\")\n",
    "            torch.save(parser.model.state_dict(), output_path)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
    "    \n",
    "    parser.model.train()  # Places model in \"train\" mode, i.e. apply dropout layer\n",
    "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    with tqdm(total=(n_minibatches)) as prog:\n",
    "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
    "            \n",
    "            #train_x:  batch_size, n_features\n",
    "            #train_y:  batch_size, target(=3)\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            loss = 0.\n",
    "            train_x = torch.from_numpy(train_x).long()  #long() for int so embedding works....\n",
    "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()  #get the index with 1 because torch expects label to be single integer\n",
    "\n",
    "            # Forward pass: compute predicted logits.\n",
    "            logits = parser.model(train_x)\n",
    "            # Compute loss\n",
    "            loss = loss_func(logits, train_y)\n",
    "            # Compute gradients of the loss w.r.t model parameters.\n",
    "            loss.backward()\n",
    "            # Take step with optimizer.\n",
    "            optimizer.step()\n",
    "\n",
    "            prog.update(1)\n",
    "            loss_meter.update(loss.item())\n",
    "\n",
    "    print(\"Average Train Loss: {}\".format(loss_meter.avg))\n",
    "    print(\"Evaluating on dev set\",)\n",
    "    parser.model.eval()  # Places model in \"eval\" mode, i.e. don't apply dropout layer\n",
    "        \n",
    "    dev_UAS, _ = parser.parse(dev_data)\n",
    "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
    "    return dev_UAS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:10<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.9585734674086174\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7810302.64it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 56.77\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.3375779241323471\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7546096.15it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 64.07\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.27086972010632354\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 1717038.70it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 67.46\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.2357727394749721\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7655958.73it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 69.74\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:09<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.20498496728638807\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6963080.56it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 72.39\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:09<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.18123073410242796\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6919424.88it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 73.30\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.16281057552744946\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7567074.44it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 74.44\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:09<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.14941155289610228\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7014026.76it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 75.44\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:09<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1351280032346646\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7053580.60it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 75.46\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:09<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.12386445747688413\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7070193.34it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 76.72\n",
      "New best dev UAS! Saving model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create directory if it does not exist for saving the weights...\n",
    "output_dir = \"output/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
    "output_path = output_dir + \"model.weights\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "    \n",
    "model = ParserModel(embeddings_matrix)\n",
    "parser.model = model\n",
    "\n",
    "start = time.time()\n",
    "train(parser, train_examples, dev_set, output_path,\n",
    "      batch_size=1024, n_epochs=10, lr=0.0005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7886042.03it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- test UAS: 77.66\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(80 * \"=\")\n",
    "print(\"TESTING\")\n",
    "print(80 * \"=\")\n",
    "\n",
    "print(\"Restoring the best model weights found on the dev set\")\n",
    "parser.model.load_state_dict(torch.load(output_path))\n",
    "print(\"Final evaluation on test set\",)\n",
    "parser.model.eval()\n",
    "UAS, dependencies = parser.parse(test_set)\n",
    "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1. Ablation Study"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do something called ablation study (meaning try to delete something so we know the impact of that deleted thing - very common in NLP)\n",
    "- Recall that we have 18 word + 18 pos + 12 dep features.\n",
    "- Try to delete only the 12 dep features and check UAS.\n",
    "- Try to delete only the 18 pos features and check UAS."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that pretrained embedding with Penn Treebank dataset has test UAS score of 76.44. Now, let's do ablation study by deleting dep features and pos features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_PREFIX = '<p>:' #indicating pos tags\n",
    "D_PREFIX = '<d>:' #indicating dependency tags\n",
    "UNK      = '<UNK>'\n",
    "NULL     = '<NULL>'\n",
    "ROOT     = '<ROOT>'\n",
    "\n",
    "class ModifiedParser(object):\n",
    "\n",
    "    def __init__(self, dataset, is_pos=True, is_dep=True):\n",
    "        self.n_features = 18\n",
    "        self.is_pos = is_pos\n",
    "        self.is_dep = is_dep\n",
    "        trans = ['L', 'R', 'S']\n",
    "        tok2id = {}\n",
    "        self.n_trans = len(trans)\n",
    "        self.tran2id = {t: i for (i, t) in enumerate(trans)}  #use for easy coding\n",
    "        self.id2tran = {i: t for (i, t) in enumerate(trans)}\n",
    "        self.n_deprel = 1   #because we are not predicting the relations, we are only predicting S, L, R\n",
    "            #set the root dep\n",
    "        if self.is_dep == True:\n",
    "            self.root_dep = 'root'\n",
    "\n",
    "            #get all the dep of the dataset as list, e.g., ['root', 'acl', 'nmod', 'nmod:npmod']\n",
    "            all_dep = [self.root_dep] + list(set([w for ex in dataset\n",
    "                                                   for w in ex['dep']\n",
    "                                                   if w != self.root_dep]))\n",
    "        \n",
    "        #1. put dep into tok2id lookup table, with D_PREFIX so we know it is dependency\n",
    "        #{'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'D_PREFIX:<NULL>': 30}\n",
    "            tok2id = {D_PREFIX + l: i for (i, l) in enumerate(all_dep)}\n",
    "            tok2id[D_PREFIX + NULL] = self.D_NULL = len(tok2id)\n",
    "        \n",
    "        #we are using \"unlabeled\" where we do not label with the dependency\n",
    "        #thus the number of dependency relation is 1\n",
    "              \n",
    "            self.n_features += 12\n",
    "        #create a simple lookup table mapping action and id\n",
    "        #e.g., tran2id: {'L': 0, 'R': 1, 'S': 2}\n",
    "        #e.g., id2tran: {0: 'L', 1: 'R', 2: 'S'}\n",
    "        \n",
    "        #2. put pos tags into tok2id lookup table, with P_PREFIX so we know it is pos\n",
    "        if self.is_pos == True:\n",
    "            tok2id.update(build_dict([P_PREFIX + w for ex in dataset for w in ex['pos']],\n",
    "                                  offset=len(tok2id)))\n",
    "            tok2id[P_PREFIX + UNK]  = self.P_UNK  = len(tok2id)  #also remember the pos tags of unknown\n",
    "            tok2id[P_PREFIX + NULL] = self.P_NULL = len(tok2id)\n",
    "            tok2id[P_PREFIX + ROOT] = self.P_ROOT = len(tok2id)\n",
    "            \n",
    "            self.n_features += 18\n",
    "        #now tok2id:  {'P_PREFIX:root': 0, 'P_PREFIX:acl': 1, ..., 'P_PREFIX:JJR': 62, 'P_PREFIX:<UNK>': 63, 'P_PREFIX:<NULL>': 64, 'P_PREFIX:<ROOT>': 65}\n",
    "        \n",
    "        #3. put word into tok2id lookup table\n",
    "        tok2id.update(build_dict([w for ex in dataset for w in ex['word']],\n",
    "                                  offset=len(tok2id)))\n",
    "        tok2id[UNK]  = self.UNK = len(tok2id)\n",
    "        tok2id[NULL] = self.NULL = len(tok2id)\n",
    "        tok2id[ROOT] = self.ROOT = len(tok2id)\n",
    "        \n",
    "        #now tok2id: {'D_PREFIX:root': 0, 'D_PREFIX:acl': 1, 'D_PREFIX:nmod': 2, ..., 'memory': 340, 'mr.': 341, '<UNK>': 342, '<NULL>': 343, '<ROOT>': 344}\n",
    "        \n",
    "        #create id2tok\n",
    "        self.tok2id = tok2id\n",
    "        self.id2tok = {v: k for (k, v) in tok2id.items()}\n",
    "        \n",
    "        \n",
    "        self.n_tokens = len(tok2id)\n",
    "        \n",
    "    #utility function, in case we want to convert token to id\n",
    "    #function to turn train set with words to train set with id instead using tok2id\n",
    "    def numericalize(self, examples):\n",
    "        numer_examples = []\n",
    "        pos = []\n",
    "        dep = []\n",
    "        for ex in examples:\n",
    "            word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
    "                                  else self.UNK for w in ex['word']]\n",
    "\n",
    "            if self.is_pos == True:\n",
    "                    pos  = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
    "                                       else self.P_UNK for w in ex['pos']]\n",
    "            head = [-1] + ex['head']\n",
    "\n",
    "            if self.is_dep == True:\n",
    "                    dep  = [-1] + [self.tok2id[D_PREFIX + w] if D_PREFIX + w in self.tok2id\n",
    "                                else -1 for w in ex['dep']]\n",
    "            numer_examples.append({'word': word, 'pos': pos,\n",
    "                                 'head': head, 'dep': dep})\n",
    "        return numer_examples\n",
    "    \n",
    "    #function to extract features to form a feature embedding matrix\n",
    "    def extract_features(self, stack, buf, arcs, ex):\n",
    "             \n",
    "        #ex['word']:  [55, 32, 33, 34, 35, 30], i.e., ['root', 'ms.', 'haag', 'plays', 'elianti', '.']\n",
    "        #ex['pos']:   [29, 14, 14, 16, 14, 17], i.e., ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
    "        #ex['head']:  [-1, 2, 3, 0, 3, 3]  or ['root', 'compound', 'nsubj', 'root', 'dobj', 'punct']}\n",
    "        #ex['dep']:   [-1, 1, 2, 0, 6, 12] or ['compound', 'nsubj', 'root', 'dobj', 'punct']\n",
    "\n",
    "        #stack     :  [0]\n",
    "        #buffer    :  [1, 2, 3, 4, 5]\n",
    "        \n",
    "        if stack[0] == \"ROOT\":\n",
    "            stack[0] = 0  #start the stack with [ROOT]\n",
    "            \n",
    "        p_features = [] #pos features (2a, 2b, 2c) - 18\n",
    "        d_features = [] #dep features (3b, 3c) - 12\n",
    "        \n",
    "        #last 3 things on the stack as features\n",
    "        #if the stack is less than 3, then we simply append NULL from the left\n",
    "        features = [self.NULL] * (3 - len(stack)) + [ex['word'][x] for x in stack[-3:]]\n",
    "        \n",
    "        # next 3 things on the buffer as features\n",
    "        #if the buffer is less than 3, simply append NULL\n",
    "        #the reason why NULL is appended on end because buffer is read left to right\n",
    "        features += [ex['word'][x] for x in buf[:3]] + [self.NULL] * (3 - len(buf))\n",
    "        \n",
    "        #corresponding pos tags\n",
    "        if self.is_pos == True:\n",
    "            p_features = [self.P_NULL] * (3 - len(stack)) + [ex['pos'][x] for x in stack[-3:]]\n",
    "            p_features += [ex['pos'][x] for x in buf[:3]] + [self.P_NULL] * (3 - len(buf))\n",
    "        \n",
    "        #get leftmost children based on the dependency arcs\n",
    "        def get_lc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] < k])\n",
    "\n",
    "        #get right most children based on the dependency arcs\n",
    "        def get_rc(k):\n",
    "            return sorted([arc[1] for arc in arcs if arc[0] == k and arc[1] > k],\n",
    "                          reverse=True)\n",
    "        \n",
    "        #get the leftmost and rightmost children of the top two words, thus we loop 2 times\n",
    "        for i in range(2):\n",
    "            if i < len(stack):\n",
    "                k = stack[-i-1] #-1, -2 last two in the stack\n",
    "                \n",
    "                #the first and second lefmost/rightmost children of the top two words (i=1, 2) on the stack\n",
    "                lc = get_lc(k)  \n",
    "                rc = get_rc(k)\n",
    "                \n",
    "                #the leftmost of leftmost/rightmost of rightmost children of the top two words on the stack:\n",
    "                llc = get_lc(lc[0]) if len(lc) > 0 else []\n",
    "                rrc = get_rc(rc[0]) if len(rc) > 0 else []\n",
    "\n",
    "                #(leftmost of first word on stack, rightmost of first word, \n",
    "                # leftmost of the second word on stack, rightmost of second, \n",
    "                # leftmost of leftmost, rightmost of rightmost\n",
    "                features.append(ex['word'][lc[0]] if len(lc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rc[0]] if len(rc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][lc[1]] if len(lc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][rc[1]] if len(rc) > 1 else self.NULL)\n",
    "                features.append(ex['word'][llc[0]] if len(llc) > 0 else self.NULL)\n",
    "                features.append(ex['word'][rrc[0]] if len(rrc) > 0 else self.NULL)\n",
    "\n",
    "                if self.is_pos == True:\n",
    "                        #corresponding pos\n",
    "                        p_features.append(ex['pos'][lc[0]] if len(lc) > 0 else self.P_NULL)\n",
    "                        p_features.append(ex['pos'][rc[0]] if len(rc) > 0 else self.P_NULL)\n",
    "                        p_features.append(ex['pos'][lc[1]] if len(lc) > 1 else self.P_NULL)\n",
    "                        p_features.append(ex['pos'][rc[1]] if len(rc) > 1 else self.P_NULL)\n",
    "                        p_features.append(ex['pos'][llc[0]] if len(llc) > 0 else self.P_NULL)\n",
    "                        p_features.append(ex['pos'][rrc[0]] if len(rrc) > 0 else self.P_NULL)\n",
    "\n",
    "                if self.is_dep == True:\n",
    "                        #corresponding dep\n",
    "                        d_features.append(ex['dep'][lc[0]] if len(lc) > 0 else self.D_NULL)\n",
    "                        d_features.append(ex['dep'][rc[0]] if len(rc) > 0 else self.D_NULL)\n",
    "                        d_features.append(ex['dep'][lc[1]] if len(lc) > 1 else self.D_NULL)\n",
    "                        d_features.append(ex['dep'][rc[1]] if len(rc) > 1 else self.D_NULL)\n",
    "                        d_features.append(ex['dep'][llc[0]] if len(llc) > 0 else self.D_NULL)\n",
    "                        d_features.append(ex['dep'][rrc[0]] if len(rrc) > 0 else self.D_NULL)\n",
    "                \n",
    "            else:\n",
    "                #attach NULL when they don't exist\n",
    "                features += [self.NULL] * 6\n",
    "                if self.is_pos == True:\n",
    "                        p_features += [self.P_NULL] * 6\n",
    "                        \n",
    "                if self.is_dep == True:\n",
    "                        d_features += [self.D_NULL] * 6\n",
    "                        \n",
    "        features += p_features + d_features\n",
    "        #print(f\"features:{len(features)},d features:{len(d_features)},p features:{len(p_features)}, features:{self.n_features}\")\n",
    "        assert len(features) == self.n_features  #assert they are 18 + 18 + 12\n",
    "        \n",
    "        return features\n",
    "\n",
    "    #generate training examples\n",
    "    #from the training sentences and their gold parse trees \n",
    "    def create_instances(self, examples):  #examples = word, pos, head, dep\n",
    "        all_instances = []\n",
    "        \n",
    "        for i, ex in enumerate(examples):\n",
    "            #Ms. Haag plays Elianti .\n",
    "            #e.g., ex['word]: [344, 163, 99, 164, 165, 68]\n",
    "            #here 344 stands for ROOT\n",
    "            #Chaky - I cheated and take a look\n",
    "            n_words = len(ex['word']) - 1  #excluding the root\n",
    "            \n",
    "            #arcs = {(head, tail, dependency label)}\n",
    "            stack = [0]\n",
    "            buf = [i + 1 for i in range(n_words)]  #[1, 2, 3, 4, 5]\n",
    "            arcs = []\n",
    "            instances = []\n",
    "            \n",
    "            #because that's the maximum number of shift, leftarcs, rightarcs you can have\n",
    "            #this will determine the sample size of each training example\n",
    "            #if given five words, we will get a sample of (10, 48) where 10 comes from 5 * 2, and 48 is n_features\n",
    "            #but this for loop can be break if there is nothing left....\n",
    "            for i in range(n_words * 2):  #maximum times you can do either S, L, R\n",
    "                \n",
    "                #get the gold transition based on the parse trees\n",
    "                #gold_t can be either shift(2), leftarc(0), or rightarc(1)\n",
    "                gold_t = self.get_oracle(stack, buf, ex)\n",
    "                \n",
    "                #if gold_t is None, no need to extract features.....\n",
    "                if gold_t is None:\n",
    "                    break\n",
    "                \n",
    "                #make sure when the model predicts, we inform the current state of stack and buffer, so\n",
    "                #the model is not allowed to make any illegal action, e.g., buffer is empty but trying to pop\n",
    "                legal_labels = self.legal_labels(stack, buf)                \n",
    "                assert legal_labels[gold_t] == 1\n",
    "                \n",
    "                #extract the needed features\n",
    "                features = self.extract_features(stack, buf, arcs, ex)\n",
    "                instances.append((features, legal_labels, gold_t))\n",
    "                \n",
    "                #shift \n",
    "                if gold_t == 2:\n",
    "                    stack.append(buf[0])\n",
    "                    buf = buf[1:]\n",
    "                #left arc \n",
    "                elif gold_t == 0:\n",
    "                    arcs.append((stack[-1], stack[-2], gold_t))\n",
    "                    stack = stack[:-2] + [stack[-1]]\n",
    "                #right arc\n",
    "                else:\n",
    "                    arcs.append((stack[-2], stack[-1], gold_t - self.n_deprel))\n",
    "                    stack = stack[:-1]\n",
    "                    \n",
    "            else:\n",
    "                all_instances += instances\n",
    "\n",
    "        if self.is_dep == False: # We only need the word and pos\n",
    "            all_instances = [[instance[0], instance[2]] for instance in all_instances]\n",
    "\n",
    "        return all_instances\n",
    "    \n",
    "    #provide an one hot encoding of the labels\n",
    "    def legal_labels(self, stack, buf):\n",
    "        labels =  ([1] if len(stack) > 2  else [0]) * self.n_deprel  #left arc but you cannot do ROOT <--- He\n",
    "        labels += ([1] if len(stack) >= 2 else [0]) * self.n_deprel  #right arc because ROOT --> He\n",
    "        labels += [1] if len(buf) > 0 else [0]  #shift\n",
    "        return labels\n",
    "    \n",
    "    #a simple function to check punctuation POS tags\n",
    "    def punct(self, pos):\n",
    "        return pos in [\"''\", \",\", \".\", \":\", \"``\", \"-LRB-\", \"-RRB-\"]\n",
    "    \n",
    "    #decide whether to shift, leftarc, or rightarc, based on gold parse trees\n",
    "    #this is needed to create training examples which contain samples and ground truth\n",
    "    def get_oracle(self, stack, buf, ex):\n",
    "        \n",
    "        #leave if the stack is only 1, thus nothing to predict....\n",
    "        if len(stack) < 2:\n",
    "            return self.n_trans - 1\n",
    "        \n",
    "        #predict based on the last two words on the stack\n",
    "        #stack: [ROOT, he, has]\n",
    "        i0 = stack[-1] #has\n",
    "        i1 = stack[-2] #he\n",
    "        \n",
    "        #get the head and dependency\n",
    "        h0 = ex['head'][i0]\n",
    "        h1 = ex['head'][i1]\n",
    "\n",
    "        if self.is_dep == True:\n",
    "            d0 = ex['dep'][i0]\n",
    "            d1 = ex['dep'][i1]\n",
    "        \n",
    "        #either shift, left arc or right arc\n",
    "        #\"Shift\" = 2; \"LA\" = 0; \"RA\" = 1\n",
    "        #if head of the second last word is the last word, then leftarc\n",
    "        if (i1 > 0) and (h1 == i0):\n",
    "            return 0  #action is left arc ---> gold_t\n",
    "        #if head of the last word is the second last word, then rightarc\n",
    "        #make sure nothing in the buffer has head with the last word on the stack\n",
    "        #otherwise, we lose the last word.....\n",
    "        elif (i1 >= 0) and (h0 == i1) and \\\n",
    "                (not any([x for x in buf if ex['head'][x] == i0])):\n",
    "            return 1  #right arc\n",
    "        #otherwise shift, if something is left in buffer, otherwise, do nothing....\n",
    "        else:\n",
    "            return None if len(buf) == 0 else 2  #shift\n",
    "        \n",
    "    def parse(self, dataset, eval_batch_size=5000):\n",
    "        sentences = []\n",
    "        sentence_id_to_idx = {}\n",
    "        \n",
    "        for i, example in enumerate(dataset):\n",
    "            \n",
    "            #example['word']=[188, 186, 186, ..., 59]\n",
    "            #n_words=37\n",
    "            #sentence=[1, 2, 3, 4, 5,.., 37]\n",
    "            \n",
    "            n_words = len(example['word']) - 1\n",
    "            sentence = [j + 1 for j in range(n_words)]            \n",
    "            sentences.append(sentence)\n",
    "            \n",
    "            #mapping the object unique id to the i            \n",
    "            #The id is the object's memory address\n",
    "            sentence_id_to_idx[id(sentence)] = i\n",
    "            \n",
    "        model = ModelWrapper(self, dataset, sentence_id_to_idx)\n",
    "        dependencies = minibatch_parse(sentences, model, eval_batch_size)\n",
    "        \n",
    "        UAS = all_tokens = 0.0\n",
    "        with tqdm(total=len(dataset)) as prog:\n",
    "            for i, ex in enumerate(dataset):\n",
    "                head = [-1] * len(ex['word'])\n",
    "                for h, t, in dependencies[i]:\n",
    "                    head[t] = h\n",
    "                \n",
    "                if (self.is_pos == True) and (self.is_dep == True):\n",
    "                    for pred_h, gold_h, gold_l, pos in \\\n",
    "                            zip(head[1:], ex['head'][1:], ex['dep'][1:], ex['pos'][1:]):\n",
    "                            assert self.id2tok[pos].startswith(P_PREFIX)\n",
    "                            pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
    "                            if (not self.punct(pos_str)):\n",
    "                                UAS += 1 if pred_h == gold_h else 0\n",
    "                                all_tokens += 1\n",
    "\n",
    "                elif (self.is_pos == False) and (self.is_dep == True):\n",
    "                    for pred_h, gold_h, gold_l in \\\n",
    "                            zip(head[1:], ex['head'][1:], ex['dep'][1:]):\n",
    "                            UAS += 1 if pred_h == gold_h else 0\n",
    "                            all_tokens += 1\n",
    "\n",
    "                elif (self.is_pos == True) and (self.is_dep == False):\n",
    "                    for pred_h, gold_h, pos in \\\n",
    "                            zip(head[1:], ex['head'][1:], ex['pos'][1:]):\n",
    "                            assert self.id2tok[pos].startswith(P_PREFIX)\n",
    "                            pos_str = self.id2tok[pos][len(P_PREFIX):]\n",
    "                            if (not self.punct(pos_str)):\n",
    "                                UAS += 1 if pred_h == gold_h else 0\n",
    "                                all_tokens += 1\n",
    "            \n",
    "                prog.update(i + 1)\n",
    "        print(all_tokens,UAS)\n",
    "        UAS /= all_tokens\n",
    "        return UAS, dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking train set\n",
    "for ex in train_set:\n",
    "    # print(ex)\n",
    "    for w in ex['dep']:\n",
    "        # print(w)\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing modified training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data\n",
      "2. Building parser\n",
      "took 0.07 seconds\n",
      "Word:  ['ms.', 'haag', 'plays', 'elianti', '.']\n",
      "Pos:  ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
      "Head:  [2, 3, 0, 3, 3]\n",
      "Dep:  ['compound', 'nsubj', 'root', 'dobj', 'punct']\n",
      "Word:  [5156, 304, 1364, 1002, 2144, 87]\n",
      "Pos:  [84, 42, 42, 55, 42, 46]\n",
      "Head:  [-1, 2, 3, 0, 3, 3]\n",
      "Dep:  [-1, 3, 18, 0, 23, 26]\n",
      "5. Preprocessing training data...\n",
      "took 2.19 seconds\n",
      "([5155, 5155, 5156, 91, 113, 806, 5155, 5155, 5155, 5155, 5155, 5155, 5155, 5155, 5155, 5155, 5155, 5155, 83, 83, 84, 40, 41, 42, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38], [0, 0, 1], 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_set, dev_set, test_set = load_data()\n",
    "print(\"2. Building parser\")\n",
    "start = time.time()\n",
    "parser = ModifiedParser(train_set,True,True)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "#before numericalize\n",
    "print(\"Word: \", train_set[1][\"word\"])\n",
    "print(\"Pos: \",  train_set[1][\"pos\"])\n",
    "print(\"Head: \", train_set[1][\"head\"])\n",
    "print(\"Dep: \",  train_set[1][\"dep\"])\n",
    "\n",
    "train_set = parser.numericalize(train_set)\n",
    "dev_set   = parser.numericalize(dev_set)\n",
    "test_set  = parser.numericalize(test_set)\n",
    "\n",
    "#after numericalize\n",
    "print(\"Word: \", train_set[1][\"word\"])\n",
    "print(\"Pos: \",  train_set[1][\"pos\"])\n",
    "print(\"Head: \", train_set[1][\"head\"])\n",
    "print(\"Dep: \",  train_set[1][\"dep\"])\n",
    "\n",
    "print(\"5. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser.create_instances(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "print(train_examples[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying train, train_for_epoch and minibatches functions to work without dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005, is_dep=True):\n",
    "    \n",
    "    best_dev_UAS = 0\n",
    "    \n",
    "    optimizer = optim.Adam(parser.model.parameters(), lr=0.001)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
    "        dev_UAS = train_for_epoch(\n",
    "            parser, train_data, dev_data, optimizer, loss_func, batch_size, is_dep)\n",
    "        if dev_UAS > best_dev_UAS:\n",
    "            best_dev_UAS = dev_UAS\n",
    "            print(\"New best dev UAS! Saving model.\")\n",
    "            torch.save(parser.model.state_dict(), output_path)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size, is_dep=True):\n",
    "    \n",
    "    parser.model.train()  # Places model in \"train\" mode, i.e. apply dropout layer\n",
    "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
    "    loss_meter = AverageMeter()\n",
    "\n",
    "    with tqdm(total=(n_minibatches)) as prog:\n",
    "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size, is_dep)):\n",
    "            \n",
    "            #train_x:  batch_size, n_features\n",
    "            #train_y:  batch_size, target(=3)\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            loss = 0.\n",
    "            train_x = torch.from_numpy(train_x).long()  #long() for int so embedding works....\n",
    "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()  #get the index with 1 because torch expects label to be single integer\n",
    "\n",
    "            # Forward pass: compute predicted logits.\n",
    "            logits = parser.model(train_x)\n",
    "            # Compute loss\n",
    "            loss = loss_func(logits, train_y)\n",
    "            # Compute gradients of the loss w.r.t model parameters.\n",
    "            loss.backward()\n",
    "            # Take step with optimizer.\n",
    "            optimizer.step()\n",
    "\n",
    "            prog.update(1)\n",
    "            loss_meter.update(loss.item())\n",
    "\n",
    "    print(\"Average Train Loss: {}\".format(loss_meter.avg))\n",
    "    print(\"Evaluating on dev set\",)\n",
    "    parser.model.eval()  # Places model in \"eval\" mode, i.e. don't apply dropout layer\n",
    "        \n",
    "    dev_UAS, _ = parser.parse(dev_data)\n",
    "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
    "    return dev_UAS\n",
    "\n",
    "def minibatches(data, batch_size, is_dep=True):\n",
    "    x = np.array([d[0] for d in data])\n",
    "    if is_dep == True:\n",
    "        y = np.array([d[2] for d in data])\n",
    "    else:\n",
    "        y = np.array([d[1] for d in data])\n",
    "        \n",
    "    one_hot = np.zeros((y.size, 3))\n",
    "    one_hot[np.arange(y.size), y] = 1\n",
    "    return get_minibatches([x, one_hot], batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting POS features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data\n",
      "2. Building parser\n",
      "took 0.03 seconds\n",
      "Word:  ['ms.', 'haag', 'plays', 'elianti', '.']\n",
      "Pos:  ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
      "Head:  [2, 3, 0, 3, 3]\n",
      "Dep:  ['compound', 'nsubj', 'root', 'dobj', 'punct']\n",
      "3. Loading pretrained embeddings...\n",
      "Embedding matrix shape (vocab, emb size):  (5111, 50)\n",
      "took 8.20 seconds\n",
      "4. Preprocessing training data...\n",
      "took 1.69 seconds\n"
     ]
    }
   ],
   "source": [
    "train_set, dev_set, test_set = load_data()\n",
    "\n",
    "print(\"2. Building parser\")\n",
    "start = time.time()\n",
    "parser = ModifiedParser(train_set, is_pos=False)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "#before numericalize\n",
    "print(\"Word: \", train_set[1][\"word\"])\n",
    "print(\"Pos: \",  train_set[1][\"pos\"])\n",
    "print(\"Head: \", train_set[1][\"head\"])\n",
    "print(\"Dep: \",  train_set[1][\"dep\"])\n",
    "\n",
    "train_set = parser.numericalize(train_set)\n",
    "dev_set   = parser.numericalize(dev_set)\n",
    "test_set  = parser.numericalize(test_set)\n",
    "\n",
    "print(\"3. Loading pretrained embeddings...\",)\n",
    "start = time.time()\n",
    "word_vectors = {}\n",
    "for line in open(\"en-cw.txt\").readlines():\n",
    "    we = line.strip().split() #we = word embeddings - first column: word;  the rest is embedding\n",
    "    word_vectors[we[0]] = [float(x) for x in we[1:]] #{word: [list of 50 numbers], nextword: [another list], so on...}\n",
    "    \n",
    "#create an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
    "#we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token]\n",
    "        elif token.lower() in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "print(\"4. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser.create_instances(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "#print(train_examples[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.6861955393105745\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 16243169.13it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12003.0 6088.0\n",
      "- dev UAS: 50.72\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.3408315498381853\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 18856978.93it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12003.0 6574.0\n",
      "- dev UAS: 54.77\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.28300819732248783\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 17854623.12it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12003.0 7057.0\n",
      "- dev UAS: 58.79\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.24132836672166982\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 20528979.13it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12003.0 7299.0\n",
      "- dev UAS: 60.81\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.20761669582376877\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 20290316.17it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12003.0 7511.0\n",
      "- dev UAS: 62.58\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.18509586962560812\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 20151773.22it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12003.0 7712.0\n",
      "- dev UAS: 64.25\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.16514081538965306\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 20629749.70it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12003.0 7840.0\n",
      "- dev UAS: 65.32\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.14893683372065425\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 18904479.33it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12003.0 7857.0\n",
      "- dev UAS: 65.46\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1349606312190493\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 20801289.88it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12003.0 8143.0\n",
      "- dev UAS: 67.84\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.12452341429889202\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 14315908.44it/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12003.0 8168.0\n",
      "- dev UAS: 68.05\n",
      "New best dev UAS! Saving model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create directory if it does not exist for saving the weights...\n",
    "output_dir = \"output/{:%Y%m%d_%H%M%S}_del_pos/\".format(datetime.now())\n",
    "output_path = output_dir + \"model.weights_test_del_pos\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "    \n",
    "model = ParserModel(embeddings_matrix, n_features=30) # Because now we only use 2 features (18 + 12)\n",
    "parser.model = model\n",
    "\n",
    "start = time.time()\n",
    "train(parser, train_examples, dev_set, output_path,\n",
    "      batch_size=1024, n_epochs=10, lr=0.0005)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in test_set:\n",
    "    # print(ex)\n",
    "    for w in ex['dep']:\n",
    "        # print(w)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING is_pos False\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 20953953.81it/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11890.0 8358.0\n",
      "- test UAS: 70.29\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(80 * \"=\")\n",
    "print(\"TESTING is_pos False\")\n",
    "print(80 * \"=\")\n",
    "\n",
    "print(\"Restoring the best model weights found on the dev set\")\n",
    "parser.model.load_state_dict(torch.load(output_path))\n",
    "print(\"Final evaluation on test set\",)\n",
    "parser.model.eval()\n",
    "UAS, dependencies = parser.parse(test_set)\n",
    "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Dep features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data\n",
      "2. Building parser\n",
      "took 0.11 seconds\n",
      "Word:  ['ms.', 'haag', 'plays', 'elianti', '.']\n",
      "Pos:  ['NNP', 'NNP', 'VBZ', 'NNP', '.']\n",
      "Head:  [2, 3, 0, 3, 3]\n",
      "Dep:  ['compound', 'nsubj', 'root', 'dobj', 'punct']\n",
      "3. Loading pretrained embeddings...\n",
      "Embedding matrix shape (vocab, emb size):  (5118, 50)\n",
      "took 9.35 seconds\n",
      "4. Preprocessing training data...\n",
      "took 2.72 seconds\n"
     ]
    }
   ],
   "source": [
    "train_set, dev_set, test_set = load_data()\n",
    "\n",
    "print(\"2. Building parser\")\n",
    "start = time.time()\n",
    "parser = ModifiedParser(train_set, is_dep=False)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "#before numericalize\n",
    "print(\"Word: \", train_set[1][\"word\"])\n",
    "print(\"Pos: \",  train_set[1][\"pos\"])\n",
    "print(\"Head: \", train_set[1][\"head\"])\n",
    "print(\"Dep: \",  train_set[1][\"dep\"])\n",
    "\n",
    "train_set = parser.numericalize(train_set)\n",
    "dev_set   = parser.numericalize(dev_set)\n",
    "test_set  = parser.numericalize(test_set)\n",
    "\n",
    "print(\"3. Loading pretrained embeddings...\",)\n",
    "start = time.time()\n",
    "word_vectors = {}\n",
    "for line in open(\"en-cw.txt\").readlines():\n",
    "    we = line.strip().split() #we = word embeddings - first column: word;  the rest is embedding\n",
    "    word_vectors[we[0]] = [float(x) for x in we[1:]] #{word: [list of 50 numbers], nextword: [another list], so on...}\n",
    "    \n",
    "#create an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
    "#we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token]\n",
    "        elif token.lower() in word_vectors:\n",
    "            embeddings_matrix[i] = word_vectors[token.lower()]\n",
    "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "print(\"4. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser.create_instances(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "#print(train_examples[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.7239006708065668\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 9458706.81it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10552.0 5902.0\n",
      "- dev UAS: 55.93\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:02<00:00, 22.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.3124919260541598\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6604933.25it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10552.0 6645.0\n",
      "- dev UAS: 62.97\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:02<00:00, 23.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.25332304649055004\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 10860119.82it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10552.0 6925.0\n",
      "- dev UAS: 65.63\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:01<00:00, 25.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.21653803624212742\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 11907803.70it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10552.0 7097.0\n",
      "- dev UAS: 67.26\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:01<00:00, 24.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.19049131715049347\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 8580285.76it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10552.0 7373.0\n",
      "- dev UAS: 69.87\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:02<00:00, 23.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1680047751093904\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 9428151.04it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10552.0 7432.0\n",
      "- dev UAS: 70.43\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:02<00:00, 23.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.15052838996052742\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7191268.90it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10552.0 7611.0\n",
      "- dev UAS: 72.13\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:01<00:00, 24.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.13598615784818927\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7737599.43it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10552.0 7718.0\n",
      "- dev UAS: 73.14\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 11.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.12558599328622222\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6848791.81it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10552.0 7813.0\n",
      "- dev UAS: 74.04\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.11312271514907479\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6150329.87it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10552.0 7735.0\n",
      "- dev UAS: 73.30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#create directory if it does not exist for saving the weights...\n",
    "output_dir = \"output/{:%Y%m%d_%H%M%S}_del_dep/\".format(datetime.now())\n",
    "output_path = output_dir + \"model.weights_test_del_dep\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "    \n",
    "model = ParserModel(embeddings_matrix, n_features=36) # Because now we only use 2 features (18 + 12)\n",
    "parser.model = model\n",
    "\n",
    "start = time.time()\n",
    "train(parser, train_examples, dev_set, output_path,\n",
    "      batch_size=1024, n_epochs=10, lr=0.0005, is_dep=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING is_dep False\n",
      "================================================================================\n",
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6013055.15it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10512.0 7892.0\n",
      "- test UAS: 75.08\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(80 * \"=\")\n",
    "print(\"TESTING is_dep False\")\n",
    "print(80 * \"=\")\n",
    "\n",
    "print(\"Restoring the best model weights found on the dev set\")\n",
    "parser.model.load_state_dict(torch.load(output_path))\n",
    "print(\"Final evaluation on test set\",)\n",
    "parser.model.eval()\n",
    "UAS, dependencies = parser.parse(test_set)\n",
    "print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Comparison Study: Testing Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data\n"
     ]
    }
   ],
   "source": [
    "train_set, dev_set, test_set = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ms.', 'haag', 'plays', 'elianti', '.']\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "corpus_tokenized = [word['word'] for word in train_set]\n",
    "\n",
    "# Check results\n",
    "print(corpus_tokenized[1])\n",
    "print(len(corpus_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['election', 'auto', 'oakland', 'laboratory', '911,606']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs = list(set(flatten(corpus_tokenized)))\n",
    "vocabs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Index\n",
    "word2index = {}\n",
    "word2index['<UNK>'] = 0 \n",
    "for index, vocab in enumerate(vocabs):\n",
    "    word2index[vocab] = index + 1\n",
    "vocabs.append('<UNK>')\n",
    "\n",
    "#Index2Word\n",
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n"
     ]
    }
   ],
   "source": [
    "# Build co-occurance matrix\n",
    "from collections import Counter\n",
    "X_i = Counter(flatten(corpus_tokenized)) # X_i\n",
    "print(X_i['is'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skipgrams(corpus, window_size=1):\n",
    "    \n",
    "    skipgrams = []\n",
    "    for sentence in corpus:\n",
    "        for i in range(1, len(sentence) - 1): \n",
    "            center_word = sentence[i]\n",
    "            outside_word = []\n",
    "            for ws in range(window_size):\n",
    "                #check if it is outside of range of the list\n",
    "                if i + 1 + ws < len(sentence):\n",
    "                    outside_word.append(sentence[i+ws+1])\n",
    "\n",
    "                if i - ws - 1 >= 0:\n",
    "                    outside_word.append(sentence[(i - ws - 1)])\n",
    "\n",
    "            for o in outside_word:\n",
    "                skipgrams.append(((center_word, o)))\n",
    "\n",
    "    return skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find co-occurance in skip_grams with window of 2\n",
    "X_ik_skipgram = Counter(create_skipgrams(corpus_tokenized, window_size=2))\n",
    "\n",
    "# X_ik_skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgrams = create_skipgrams(corpus_tokenized, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight function\n",
    "\n",
    "#simply a normalized function...don't worry too much\n",
    "def weighting(w_i, w_j, X_ik):\n",
    "        \n",
    "    #check whether the co-occurrences exist between these two words\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "    except:\n",
    "        x_ij = 1  #if does not exist, set it to 1, basically smoothing technique\n",
    "                \n",
    "    x_max = 100 #100 # fixed in paper  #cannot exceed 100 counts\n",
    "    alpha = 0.75 # Followed Chaky way!\n",
    "    \n",
    "    #if co-occurrence does not exceed 100, scale it based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max)**alpha  #scale it\n",
    "    else:\n",
    "        result = 1  #if is greater than max, set it to 1 maximum\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 12854985/25704900 [01:15<01:15, 169647.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "from tqdm import tqdm\n",
    "\n",
    "X_ik = {}  #for keeping the co-occurences\n",
    "weighting_dic = {} #scaling the percentage of sampling\n",
    "with tqdm(total=len(vocabs) ** 2) as prog:\n",
    "    for bigram in combinations_with_replacement(vocabs, 2):\n",
    "        if X_ik_skipgram.get(bigram) is not None:  #matches \n",
    "            cooc = X_ik_skipgram[bigram]  #get the count from what we already counted\n",
    "            X_ik[bigram] = cooc + 1 # + 1 for stability issue\n",
    "            X_ik[(bigram[1],bigram[0])] = cooc + 1   #count also for the opposite\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        prog.update(1)\n",
    "        weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "        weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def random_batch(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_coocs  = []\n",
    "    random_weightings = []\n",
    "    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams_id[i][1]])  # context word, e.g., 3\n",
    "        \n",
    "        #get cooc\n",
    "        pair = skip_grams[i]\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "        \n",
    "        #get weighting\n",
    "        #print(pair)\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append([weighting])\n",
    "                    \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[4557]\n",
      " [1359]]\n",
      "Target:  [[3559]\n",
      " [4065]]\n",
      "Cooc:  [[1.09861229]\n",
      " [1.09861229]]\n",
      "Weighting:  [[0.07208434]\n",
      " [0.07208434]]\n"
     ]
    }
   ],
   "source": [
    "#testing the method\n",
    "batch_size = 2 # mini-batch size\n",
    "skip_grams = create_skipgrams(corpus_tokenized,window_size=2)\n",
    "input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, corpus_tokenized, skip_grams, X_ik, weighting_dic)\n",
    "\n",
    "print(\"Input: \", input_batch)\n",
    "print(\"Target: \", target_batch)\n",
    "print(\"Cooc: \", cooc_batch)\n",
    "print(\"Weighting: \", weighting_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,embed_size):\n",
    "        super(GloVe,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size) # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size) # out embedding\n",
    "        \n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "    def forward(self, center_words, target_words, coocs, weighting):\n",
    "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_u(target_words) # [batch_size, 1, emb_size]\n",
    "        \n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "        \n",
    "        inner_product = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        #note that coocs already got log\n",
    "        loss = weighting*torch.pow(inner_product +center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare parameters\n",
    "voc_size = len(vocabs)\n",
    "batch_size     = 10 # mini-batch size\n",
    "embedding_size = 2\n",
    "model          = GloVe(voc_size, embedding_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 502/5000 [00:30<04:54, 15.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500 | cost: 36.765152 | time: 0m 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1002/5000 [00:59<03:37, 18.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | cost: 69.036018 | time: 0m 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1501/5000 [01:26<03:39, 15.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1500 | cost: 3.658460 | time: 0m 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2003/5000 [01:50<01:55, 25.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 | cost: 54.769108 | time: 0m 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2503/5000 [02:10<01:44, 23.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2500 | cost: 41.982983 | time: 0m 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3002/5000 [02:43<01:58, 16.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3000 | cost: 7.589264 | time: 0m 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3503/5000 [03:15<01:31, 16.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3500 | cost: 1.228089 | time: 0m 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4002/5000 [03:49<01:11, 14.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4000 | cost: 5.695064 | time: 0m 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4502/5000 [04:24<00:30, 16.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4500 | cost: 2.689601 | time: 0m 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [04:57<00:00, 16.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 | cost: 1.519522 | time: 0m 0s\n",
      "Total time: 4m 57s\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_training = time.time()\n",
    "# Training\n",
    "num_epochs = 5000\n",
    "with tqdm(total=num_epochs) as prog:\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, corpus_tokenized, skip_grams, X_ik, weighting_dic)\n",
    "        input_batch  = torch.LongTensor(input_batch)         #[batch_size, 1]\n",
    "        target_batch = torch.LongTensor(target_batch)       #[batch_size, 1]\n",
    "        cooc_batch   = torch.FloatTensor(cooc_batch)         #[batch_size, 1]\n",
    "        weighting_batch = torch.FloatTensor(weighting_batch) #[batch_size, 1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "        \n",
    "\n",
    "        if (epoch + 1) % 500 == 0:\n",
    "            print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n",
    "        \n",
    "        prog.update(1)\n",
    "\n",
    "end_training = time.time()\n",
    "start_min, end_min = epoch_time(start_training, end_training)\n",
    "print(f'Total time: {start_min}m {end_min}s\")')\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"GloVE_5000.pkl.pth\")\n",
    "with open(\"GloVE_5000.pkl\",'wb') as f:\n",
    "    pickle.dump(model,f)\n",
    "    # break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some functions\n",
    "#numpy version\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim\n",
    "\n",
    "# Function to get embedding\n",
    "def get_embed(word, current_model=model):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except :\n",
    "        index = word2index['<UNK>'] #unknown\n",
    "    word = torch.LongTensor([index])\n",
    "    \n",
    "    embed =  (current_model.embedding_v(word)+current_model.embedding_u(word))/2\n",
    "    return np.array(embed[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "for i, word in enumerate(vocabs[:20]): #loop each unique vocab\n",
    "    x, y = get_embed(word)\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords='offset points')\n",
    "plt.title('Glove with window size of 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data\n",
      "2. Building parser....\n",
      "took 0.05 seconds\n"
     ]
    }
   ],
   "source": [
    "train_set, dev_set, test_set = load_data()\n",
    "print('2. Building parser....')\n",
    "start = time.time()\n",
    "parser = Parser(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time()-start))\n",
    "\n",
    "train_set = parser.numericalize(train_set)\n",
    "dev_set = parser.numericalize(dev_set)\n",
    "test_set = parser.numericalize(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = model.load_state_dict(torch.load(\"GloVe_5000.pkl.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Loading pretrained embeddings...\n",
      "Embedding matrix shape (vocab, emb size):  (5157, 50)\n",
      "took 0.02 seconds\n",
      "5. Preprocessing training data...\n",
      "took 2.30 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"4. Loading pretrained embeddings...\",)\n",
    "# config = Config()\n",
    "start = time.time()\n",
    "# word_vectors = {}\n",
    "# for line in open('./data/en-cw.txt').readlines():\n",
    "#     we = line.strip().split() #we = word embeddings - first column: word;  the rest is embedding\n",
    "#     word_vectors[we[0]] = [float(x) for x in we[1:]] #{word: [list of 50 numbers], nextword: [another list], so on...}\n",
    "    \n",
    "#create an empty embedding matrix holding the embedding lookup table (vocab size, embed dim)\n",
    "#we use random.normal instead of zeros, to keep the embedding matrix arbitrary in case word vectors don't exist....\n",
    "embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "for token in parser.tok2id:\n",
    "        i = parser.tok2id[token]\n",
    "        if token in simple_model: #Using the nn.Embedding\n",
    "            embeddings_matrix[i] = simple_model[token]\n",
    "        elif token.lower() in simple_model:\n",
    "            embeddings_matrix[i] = simple_model[token.lower()]\n",
    "print(\"Embedding matrix shape (vocab, emb size): \", embeddings_matrix.shape)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))\n",
    "\n",
    "print(\"5. Preprocessing training data...\",)\n",
    "start = time.time()\n",
    "train_examples = parser.create_instances(train_set)\n",
    "print(\"took {:.2f} seconds\".format(time.time() - start))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING\n",
      "================================================================================\n",
      "Epoch 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:10<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.8738623261451721\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7489828.57it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 50.93\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:10<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.3518253769725561\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6492449.81it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 59.64\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:09<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.28878246806561947\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6825744.19it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 63.80\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:09<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.24850885197520256\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7217351.43it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 67.43\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.22141948156058788\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6686904.30it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 68.24\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 6 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.19946797968198857\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6294245.07it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 71.04\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 7 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 11.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.18039916704098383\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 5459006.54it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 71.11\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 11.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.1674006630976995\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 7474164.16it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 72.69\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 9 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:09<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.15187376691028476\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 6956349.74it/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 73.48\n",
      "New best dev UAS! Saving model.\n",
      "\n",
      "Epoch 10 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00, 11.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: 0.14011773265277347\n",
      "Evaluating on dev set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125250it [00:00, 5936476.06it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- dev UAS: 74.10\n",
      "New best dev UAS! Saving model.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_path = \"./output/model_simple.weights\"\n",
    "\n",
    "print(80 * \"=\")\n",
    "print(\"TRAINING\")\n",
    "print(80 * \"=\")\n",
    "    \n",
    "model = ParserModel(embeddings_matrix)\n",
    "parser.model = model\n",
    "\n",
    "start = time.time()\n",
    "train(parser, train_examples, dev_set, output_path,\n",
    "      batch_size=1024, n_epochs=10, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a9409b9deca2e32b1801514822bf60b5f36ee24c0efb3dea589c0a4c1325f8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

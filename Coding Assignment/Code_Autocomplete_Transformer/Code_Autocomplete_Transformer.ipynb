{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "45c5f30186424db8aae50bb1c0e50987": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d731d46b829f4c3ba54dc8c15d5ec81d",
              "IPY_MODEL_62cc7a1792ba42e8800224e955174ad1",
              "IPY_MODEL_72d5917fc6d04c749318d6eb738e6928"
            ],
            "layout": "IPY_MODEL_be98317d686945b898337c137c7189c5"
          }
        },
        "d731d46b829f4c3ba54dc8c15d5ec81d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51e03d6d293c47ffb0ccd75f738a7c95",
            "placeholder": "​",
            "style": "IPY_MODEL_b7f1220df2dd4c849e8e78e25219ad38",
            "value": "100%"
          }
        },
        "62cc7a1792ba42e8800224e955174ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ce4c11020ac418c94011f46a19cb304",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d48ba28ed814af4b7832da4942ffe50",
            "value": 2
          }
        },
        "72d5917fc6d04c749318d6eb738e6928": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afd01d5d78c94571a6a1f32377580748",
            "placeholder": "​",
            "style": "IPY_MODEL_917d7ee3f8de4e869b9e979c7302c0ea",
            "value": " 2/2 [00:00&lt;00:00, 58.27it/s]"
          }
        },
        "be98317d686945b898337c137c7189c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51e03d6d293c47ffb0ccd75f738a7c95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7f1220df2dd4c849e8e78e25219ad38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ce4c11020ac418c94011f46a19cb304": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d48ba28ed814af4b7832da4942ffe50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "afd01d5d78c94571a6a1f32377580748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "917d7ee3f8de4e869b9e979c7302c0ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDEt2i07jn_l",
        "outputId": "39201c60-6284-4277-cced-4f85cfd1e370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.10.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.1)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchtext, datasets, math\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_dataset(\"codeparrot/github-jupyter-code-to-text\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272,
          "referenced_widgets": [
            "45c5f30186424db8aae50bb1c0e50987",
            "d731d46b829f4c3ba54dc8c15d5ec81d",
            "62cc7a1792ba42e8800224e955174ad1",
            "72d5917fc6d04c749318d6eb738e6928",
            "be98317d686945b898337c137c7189c5",
            "51e03d6d293c47ffb0ccd75f738a7c95",
            "b7f1220df2dd4c849e8e78e25219ad38",
            "1ce4c11020ac418c94011f46a19cb304",
            "4d48ba28ed814af4b7832da4942ffe50",
            "afd01d5d78c94571a6a1f32377580748",
            "917d7ee3f8de4e869b9e979c7302c0ea"
          ]
        },
        "id": "WorIyZGKjz2Q",
        "outputId": "007c970d-9918-4f42-d3e2-a69ea1770790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45c5f30186424db8aae50bb1c0e50987"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['repo_name', 'path', 'license', 'content'],\n",
            "        num_rows: 47452\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['repo_name', 'path', 'license', 'content'],\n",
            "        num_rows: 11864\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['train'][333].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK-xnuoVj1DQ",
        "outputId": "2987fab8-c4a5-4c7b-8e53-254c14d9c30a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['repo_name', 'path', 'license', 'content'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_train_dataset =[]\n",
        "total_test_dataset = []\n",
        "for text in dataset['train']['content']:\n",
        "  for split in text.split('\\n'):\n",
        "    if split != \"\":\n",
        "      total_train_dataset.append(split)\n",
        "\n",
        "for text in dataset['test']['content']:\n",
        "  for split in text.split('\\n'):\n",
        "    if split != \"\":\n",
        "      total_test_dataset.append(split)\n",
        "\n",
        "print(len(total_train_dataset),len(total_test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX_7BEssj42Z",
        "outputId": "e7452c29-cb59-4dd9-fbac-10133231294c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11367363 2875424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "train_total = 300000\n",
        "test_total = 3000\n",
        "# tokenized_dataset_train = yield_tokens(train_set[:2*train_total])\n",
        "# tokenized_dataset_test = yield_tokens(test_set[:test_total])\n",
        "tokenized_dataset_train = yield_tokens(total_train_dataset[:2*train_total])\n",
        "tokenized_dataset_test = yield_tokens(total_test_dataset[:test_total])"
      ],
      "metadata": {
        "id": "CFiMXDsej45O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import locale\n",
        "# def getpreferredencoding(do_setlocale = True):\n",
        "#     return \"UTF-8\"\n",
        "# locale.getpreferredencoding = getpreferredencoding\n",
        "# !python3 -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUhtCuBqj5E0",
        "outputId": "645898ca-ab29-4159-d9cb-7c237181e5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-11 07:34:40.049516: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-11 07:34:40.049635: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-11 07:34:40.049655: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.1/en_core_web_md-3.4.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-md==3.4.1) (3.4.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.65.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.10.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.4.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.25.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (3.0.12)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.22.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (6.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.1) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import spacy\n",
        "import re\n",
        "import spacy\n",
        "#spacy.load(\"en_core_web_s\")\n",
        "# import en_core_web_md\n",
        "# nlp = en_core_web_md.load()\n",
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "def preprocessing(sentence):\n",
        "    \n",
        "    # Clear the html tag by using regular expression.\n",
        "    sentence = re.sub(\"<[^>]*>\", \"\", sentence)\n",
        "    sentence = re.sub(\"[^\\x00-\\x7F]+\", \"\", sentence) #extract non-english out\n",
        "    #It matches any character which is not contained in the ASCII character set (0-127, i.e. 0x0 to 0x7F)\n",
        "    stopwords = list(STOP_WORDS)\n",
        "    doc = nlp(sentence)\n",
        "    cleaned_tokens = []\n",
        "    \n",
        "    for token in doc:\n",
        "        if token.text not in stopwords and token.pos_ != 'PUNCT' and token.pos_ != 'SPACE' and \\\n",
        "            token.pos_ != 'SYM' and token.pos_!= 'X':\n",
        "                cleaned_tokens.append(token.lemma_.lower().strip())\n",
        "                \n",
        "    return \" \".join(cleaned_tokens)"
      ],
      "metadata": {
        "id": "oUfFkN0zj5HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "def yield_tokens(data_iter):\n",
        "    for text in data_iter:\n",
        "        yield tokenizer(preprocessing(text))\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(total_train_dataset[:test_total]), min_freq=5) \n",
        "vocab.insert_token('<unk>', 0)           \n",
        "vocab.insert_token('<eos>', 3)            \n",
        "vocab.insert_token('<sos>', 2)            \n",
        "vocab.insert_token('<pad>', 1)               \n",
        "vocab.set_default_index(vocab['<unk>'])   \n",
        "print('Vocab Size',len(vocab))                         \n",
        "print(vocab.get_itos()[:10]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TK9Ip2Rj5Jg",
        "outputId": "9ad469c0-9b98-461d-8553-296b9dd06093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size 385\n",
            "['<unk>', '<pad>', '=', '<sos>', 'explanation', '<eos>', '#', 'end', 'import', \"'\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(dataset, vocab, batch_size):\n",
        "    data = []                                                   \n",
        "    for example in dataset:       \n",
        "        #appends eos so we know it ends....so model learn how to end...                             \n",
        "        tokens = example.append('') #end of sentence\n",
        "        #numericalize          \n",
        "        tokens = [vocab[token] for token in example] \n",
        "        data.extend(tokens)                                    \n",
        "    data = torch.LongTensor(data)                                 \n",
        "    num_batches = data.shape[0] // batch_size \n",
        "    data = data[:num_batches * batch_size]                       \n",
        "    data = data.view(batch_size, num_batches)        \n",
        "    return data"
      ],
      "metadata": {
        "id": "ETBhZa7jkUpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 12\n",
        "train_data = get_data(tokenized_dataset_train, vocab, batch_size)\n",
        "valid_data = get_data(tokenized_dataset_test, vocab, batch_size)"
      ],
      "metadata": {
        "id": "k1vsOkdHkYRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unk_idx, pad_idx, sos_idx, eos_idx = 0, 1, 2, 3\n",
        "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
      ],
      "metadata": {
        "id": "DmmVNXqokxoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads  #make sure it's divisible....\n",
        "        \n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc   = nn.Linear(hid_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale   = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, q, k, v, mask = None):\n",
        "        b = q.shape[0]\n",
        "        \n",
        "        Q = self.fc_q(q)\n",
        "        K = self.fc_k(k)\n",
        "        V = self.fc_v(v)\n",
        "        #Q, K, V = [b, l, h]\n",
        "        \n",
        "        #reshape them into head_dim\n",
        "        #reshape them to [b, n_heads, l, head_dim]\n",
        "        Q = Q.view(b, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(b, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(b, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        #Q, K, V = [b, n_heads, l, head_dim]\n",
        "        \n",
        "        #e = QK/sqrt(dk)\n",
        "        e = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        #e: [b, n_heads, ql, kl]\n",
        "        \n",
        "        if mask is not None:\n",
        "            e = e.masked_fill(mask == 0, -1e10)\n",
        "            \n",
        "        a = torch.softmax(e, dim=-1)\n",
        "        \n",
        "        #eV\n",
        "        x = torch.matmul(self.dropout(a), V)\n",
        "        #x: [b, n_heads, ql, head_dim]\n",
        "        \n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        #x: [b, ql, n_heads, head_dim]\n",
        "        \n",
        "        #concat them together\n",
        "        x = x.view(b, -1, self.hid_dim)\n",
        "        #x: [b, ql, h]\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        #x = [b, ql, h]\n",
        "        \n",
        "        return x, a"
      ],
      "metadata": {
        "id": "YLtVFQbZkaCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.dropout(torch.relu(self.fc1(x))))"
      ],
      "metadata": {
        "id": "ddjEmt8blB88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.norm_att_norm = nn.LayerNorm(hid_dim) #second green box\n",
        "        #self.enc__att_norm_ff  =  nn.LayerNorm(hid_dim) #third green box\n",
        "        self.full_att_norm = nn.LayerNorm(hid_dim)\n",
        "        #self.norm_maskedatt =  nn.LayerNorm(hid_dim) #first green box\n",
        "\n",
        "        #self.multi_masked  =  MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.self_attention   =  MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "\n",
        "        self.ff      =  PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
        "        self.dropout =  nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "        \n",
        "        #self attention\n",
        "        _trg, attention = self.self_attention(enc_src, enc_src, trg, src_mask)\n",
        "        \n",
        "        #dropout, residual connection and layer norm #-----> first layer\n",
        "        trg = self.norm_att_norm(trg + self.dropout(_trg))\n",
        "            \n",
        "        \n",
        "        \n",
        "        #positionwise feedforward  #-----> Passing into the  second layer without mask attention\n",
        "        _trg = self.ff(trg) \n",
        "        \n",
        "        #dropout, residual and layer norm # ------> passsing into third layer\n",
        "        trg = self.full_att_norm(trg + self.dropout(_trg))\n",
        "        \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #attention = [batch size, n heads, trg len, src len]\n",
        "        \n",
        "        return trg, attention"
      ],
      "metadata": {
        "id": "0yW8yvXOlNw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, output_dim, hid_dim, n_layers, n_heads,\n",
        "                 pf_dim, dropout, device, max_length = 100):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pad_idx = pad_idx\n",
        "        self.scale   = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layers  = nn.ModuleList(\n",
        "                     [\n",
        "                        DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
        "                        for _ in range(n_layers)\n",
        "                     ]\n",
        "                     )\n",
        "        self.fc      = nn.Linear(hid_dim, output_dim)\n",
        "        self.device  = device\n",
        "\n",
        "    def forward(self, trg,src, enc_src=[]):\n",
        "        #trg = [b, tl]\n",
        "        #enc_src = hidden states from encoder = [b, sl, h]\n",
        "        #trg_mask = [b, 1, tl, tl]\n",
        "        #src_mask = [b, 1, 1, sl]\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "       \n",
        "        trg_mask = (trg != self.pad_idx).unsqueeze(1).unsqueeze(2)&torch.tril(torch.ones((trg_len,trg_len), device = self.device)).bool()\n",
        "        \n",
        "        #pos\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        #pos: [b, l]\n",
        "\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        \n",
        "        # pos_emb = self.pos_emb(pos) #[b, l, h]\n",
        "        # trg_emb = self.trg_emb(trg) #[b, l, h]\n",
        "        \n",
        "        # x = pos_emb + trg_emb * self.scale #[b, l, h]\n",
        "        # x = self.dropout(x)\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, src, trg_mask, src_mask)\n",
        "        \n",
        "        output = self.fc(trg)\n",
        "        \n",
        "        return output, attention"
      ],
      "metadata": {
        "id": "44ymSq44llQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ],
      "metadata": {
        "id": "lA3wS8jhn2VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5p0MHe3n_ii",
        "outputId": "78b1acd5-962c-48a6-99af-c981bf61b134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(vocab)\n",
        "OUTPUT_DIM = len(vocab)\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "# SRC_PAD_IDX = PAD_IDX\n",
        "# TRG_PAD_IDX = PAD_IDX\n",
        "model = dec.apply(initialize_weights).to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjrZaOJAn39Q",
        "outputId": "daec5b00-0b7b-4a34-f9c9-f5dca4df9031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (pos_embedding): Embedding(100, 256)\n",
              "  (tok_embedding): Embedding(385, 256)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (layers): ModuleList(\n",
              "    (0): DecoderLayer(\n",
              "      (norm_att_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (full_att_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (self_attention): MultiHeadAttentionLayer(\n",
              "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): PositionwiseFeedforwardLayer(\n",
              "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): DecoderLayer(\n",
              "      (norm_att_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (full_att_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (self_attention): MultiHeadAttentionLayer(\n",
              "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): PositionwiseFeedforwardLayer(\n",
              "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): DecoderLayer(\n",
              "      (norm_att_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (full_att_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "      (self_attention): MultiHeadAttentionLayer(\n",
              "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): PositionwiseFeedforwardLayer(\n",
              "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=256, out_features=385, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx) #combine softmax with cross entropy\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has {num_params:,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GpW12IHn4G3",
        "outputId": "d3caf4c6-bfd4-48a1-972d-e5aae5bc0eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 1,804,417 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(data, seq_len, idx):\n",
        "    #this data is from get_data()\n",
        "    #train_data.shape # [batch_size, number of batches....]\n",
        "    src    = data[:, idx:idx+seq_len]                   \n",
        "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
        "    return src, target"
      ],
      "metadata": {
        "id": "AKNJGljAoGUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    # drop all batches that are not a multiple of seq_len\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "\n",
        "    \n",
        "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
        "        optimizer.zero_grad()\n",
        "        # hidden = model.detach_hidden(hidden)\n",
        "\n",
        "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
        "        src, target = src.to(device), target.to(device)\n",
        "        batch_size = src.shape[0]\n",
        "        # hidden = model.init_hidden(src)\n",
        "        prediction, hidden = model(target,src)               \n",
        "\n",
        "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
        "        target = target.reshape(-1)\n",
        "        loss = criterion(prediction, target)\n",
        "        print(loss)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip) #prevent gradient explosion - clip is basically \n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * seq_len\n",
        "    return epoch_loss / num_batches"
      ],
      "metadata": {
        "id": "2m3Bdh29oKiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    model.eval()\n",
        "    num_batches = data.shape[-1]\n",
        "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
        "    num_batches = data.shape[-1]\n",
        "\n",
        "    # hidden = model.init_hidden(batch_size, device)\n",
        "    try:\n",
        "      with torch.no_grad():\n",
        "          for idx in range(0, num_batches - 1, seq_len):\n",
        "              # hidden = model.detach_hidden(hidden)\n",
        "              src, target = get_batch(data, seq_len, idx)\n",
        "              src, target = src.to(device), target.to(device)\n",
        "              batch_size= src.shape[0]\n",
        "\n",
        "              #prediction, hidden = model(src, 0, target)\n",
        "              prediction, hidden = model(target,src)\n",
        "              prediction = prediction.reshape(batch_size * seq_len, -1)\n",
        "              target = target.reshape(-1)\n",
        "              itos = vocab.get_itos()\n",
        "              # print(target.size(),prediction.size())\n",
        "              # print(itos[torch.multinomial(torch.softmax(prediction[:, -1] / 1, dim=-1) , num_samples=1).item()  ])\n",
        "              \n",
        "              loss = criterion(prediction, target)\n",
        "              epoch_loss += loss.item() * seq_len\n",
        "          return epoch_loss / num_batches\n",
        "    except:\n",
        "      print(\".\")\n",
        "    "
      ],
      "metadata": {
        "id": "6ZX-VcQ8oMfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=3407):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    model.eval()\n",
        "    tokens = tokenizer(prompt)\n",
        "    indices = [vocab[t] for t in tokens]\n",
        "    batch_size = 1\n",
        "    # hidden = model.init_hidden(batch_size, device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(max_seq_len):\n",
        "            src = torch.LongTensor([indices]).to(device)\n",
        "\n",
        "            prediction, hidden = model(src,src)\n",
        "            \n",
        "            #prediction: [batch size, seq len, vocab size]\n",
        "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
        "            \n",
        "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
        "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
        "            \n",
        "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
        "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
        "                break\n",
        "#torch.multinomial(torch.softmax(prediction[:, -1] / temperature, dim=-1) , num_samples=1).item()\n",
        "            indices.append(prediction) #autoregressive, thus output becomes input\n",
        "\n",
        "    itos = vocab.get_itos()\n",
        "    tokens = [itos[i] for i in indices]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "DC8w6ezUoOVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5R9bLufCuoh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# n_epochs = 5\n",
        "# seq_len  = 12\n",
        "# clip    = 0.25\n",
        "\n",
        "# lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
        "\n",
        "# best_valid_loss = float('inf')\n",
        "\n",
        "# for epoch in range(n_epochs):\n",
        "#     train_loss = train(model, train_data, optimizer, criterion, \n",
        "#                 batch_size, seq_len, clip, device)\n",
        "#     valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
        "#                 seq_len, device)\n",
        "\n",
        "#     lr_scheduler.step(valid_loss)\n",
        "\n",
        "#     if valid_loss < best_valid_loss:\n",
        "#         best_valid_loss = valid_loss\n",
        "#         torch.save(model.state_dict(), 'predictor_weight.pt')\n",
        "#     print(f'\\tepoch: {epoch+1}')\n",
        "#     print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
        "#     print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "tOXAtbHCoQZZ",
        "outputId": "b3d08e52-bee9-4b43-b2d3-da22538a8517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-67639bfdbfed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     train_loss = train(model, train_data, optimizer, criterion, \n\u001b[0m\u001b[1;32m     11\u001b[0m                 batch_size, seq_len, clip, device)\n\u001b[1;32m     12\u001b[0m     valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
            "\u001b[0;32m<ipython-input-78-235540c68472>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer, criterion, batch_size, seq_len, clip, device)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'import pandas'\n",
        "max_seq_len = 5\n",
        "seed = 0\n",
        "        #superdiverse   more diverse\n",
        "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0] \n",
        "#sample from this distribution higher probability will get more change\n",
        "for temperature in temperatures:\n",
        "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
        "                          vocab, device, seed)\n",
        "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OBvkjWgoSbf",
        "outputId": "40964ac9-a9ec-4cee-c900-310b8b2086e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n",
            "import pandas centrality want 32 python asin\n",
            "\n",
            "0.7\n",
            "import pandas centrality tsos 32 python score\n",
            "\n",
            "0.75\n",
            "import pandas centrality tsos 32 python score\n",
            "\n",
            "0.8\n",
            "import pandas centrality tsos 32 python score\n",
            "\n",
            "1.0\n",
            "import pandas centrality tsos 32 run_setting parameter\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uei06T58o8tY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}